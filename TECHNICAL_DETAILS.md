# ğŸ”¬ æŠ€æœ¯è¯¦æƒ…æ–‡æ¡£

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»é‡‘èé‡åŒ–äº¤æ˜“ç³»ç»Ÿçš„æŠ€æœ¯å®ç°ç»†èŠ‚ã€æ¶æ„è®¾è®¡å’Œé«˜çº§åŠŸèƒ½ã€‚

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### ä¸¤é˜¶æ®µè§£è€¦æ¶æ„

```
é˜¶æ®µ1: ä»·æ ¼é¢„æµ‹ç½‘ç»œè®­ç»ƒ
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ é‡‘èæ•°æ® [180, 11]              â”‚
â”‚         â†“                       â”‚
â”‚ Transformer Encoder             â”‚
â”‚         â†“                       â”‚
â”‚ ä»·æ ¼é¢„æµ‹å¤´ â†’ æœªæ¥7å¤©ä»·æ ¼         â”‚
â”‚ ç‰¹å¾æå–å¤´ â†’ ç­–ç•¥ç‰¹å¾ [d_model]  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

é˜¶æ®µ2: ç­–ç•¥ç½‘ç»œè®­ç»ƒ
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç­–ç•¥ç‰¹å¾ [20, d_model] (å†»ç»“)   â”‚
â”‚         â†“                       â”‚
â”‚ GRUç­–ç•¥ç½‘ç»œ (20å¤©é€’å½’)          â”‚
â”‚         â†“                       â”‚
â”‚ ä»“ä½å†³ç­– â†’ 0-10ä»“ä½             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ¢¯åº¦ä¼ æ’­é—®é¢˜è§£å†³

#### åŸé—®é¢˜åˆ†æ
```python
# åŸé—®é¢˜ï¼šargmaxä¸å¯å¯¼ï¼Œé˜»æ–­æ¢¯åº¦ä¼ æ’­
expected_position = position_logits.argmax()  # âŒ æ¢¯åº¦é˜»æ–­
gru_input = cat([features, expected_position])
strategy_state = gru_cell(gru_input, strategy_state)

# å®é™…æ¢¯åº¦è·¯å¾„ï¼š
# final_loss â†’ position_logits âœ…
# position_logits â†’ expected_position âŒ (argmaxé˜»æ–­)
# expected_position â†’ gru_input âŒ
# gru_input â†’ strategy_state âŒ
```

#### è§£å†³æ–¹æ¡ˆ
```python
# ä»·æ ¼ç½‘ç»œï¼šæ ‡å‡†ç›‘ç£å­¦ä¹ 
price_loss = mse_loss(price_pred, price_target)
price_loss.backward()  # æ¢¯åº¦å®Œæ•´ä¼ æ’­

# ç­–ç•¥ç½‘ç»œï¼šåŸºäºæ”¶ç›Šçš„å¼ºåŒ–å­¦ä¹ 
strategy_loss = -relative_return + risk_cost + opportunity_cost
strategy_loss.backward()  # åªä¼˜åŒ–ç­–ç•¥å‚æ•°
```

## ğŸ§  æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 1. MLA (Multi-Head Latent Attention)

#### ä¼ ç»Ÿæ³¨æ„åŠ› vs MLA
```python
# ä¼ ç»Ÿæ³¨æ„åŠ›
Traditional: Q@K^T@V  # O(nÂ²d) å¤æ‚åº¦

# MLA
MLA: Q@(W_kv@X)^T@(W_kv@X)  # O(ndÂ²) å¤æ‚åº¦ï¼Œd<<næ—¶æ›´é«˜æ•ˆ

# K/Vå‹ç¼©ç¤ºä¾‹
original_kv_dim = 1024    # åŸå§‹ç»´åº¦
compressed_dim = 256      # å‹ç¼©åç»´åº¦
compression_ratio = 4     # å‹ç¼©æ¯”ä¾‹
```

#### MLAå®ç°ç»†èŠ‚
```python
class MLA(nn.Module):
    def __init__(self, args):
        # K/V æ½œåœ¨æŠ•å½±å‹ç¼©
        self.kv_compress = nn.Linear(args.d_model, self.kv_lora_rank, bias=False)
        self.kv_norm = RMSNorm(self.kv_lora_rank, eps=args.layer_norm_eps)
        
        # ä»å‹ç¼©è¡¨ç¤ºæ¢å¤ K/V
        self.k_up = nn.Linear(self.kv_lora_rank, self.n_heads * self.qk_head_dim, bias=False)
        self.v_up = nn.Linear(self.kv_lora_rank, self.n_heads * self.v_dim, bias=False)
        
        # æŸ¥è¯¢æŠ•å½±ï¼Œåˆ†ä¸º RoPE å’Œé RoPE éƒ¨åˆ†
        self.q_nope = nn.Linear(args.d_model, self.n_heads * self.qk_nope_dim, bias=False)
        self.q_rope = nn.Linear(args.d_model, self.n_heads * self.qk_rope_dim, bias=False)
```

### 2. RoPE (Rotary Position Embedding)

#### ä½ç½®ç¼–ç åŸç†
```python
def precompute_freqs_cis(dim: int, end: int, theta: float = 1e4):
    # è®¡ç®—é¢‘ç‡
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))
    
    # ç”Ÿæˆä½ç½®ç´¢å¼•
    t = torch.arange(end, device=freqs.device, dtype=freqs.dtype)
    
    # è®¡ç®—æ¯ä¸ªä½ç½®å’Œé¢‘ç‡çš„å¤–ç§¯
    freqs = torch.outer(t, freqs).float()
    
    # è½¬æ¢ä¸ºå¤æ•°å½¢å¼ e^(i*theta) = cos(theta) + i*sin(theta)
    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)
    
    return freqs_cis
```

### 3. SwiGLU å‰é¦ˆç½‘ç»œ

#### æ¿€æ´»å‡½æ•°å¯¹æ¯”
```python
# æ ‡å‡†FFN
FFN(x) = W2 * ReLU(W1 * x + b1) + b2

# SwiGLU
SwiGLU(x) = (Swish(W1 * x) âŠ™ W3 * x) * W2
# å…¶ä¸­ Swish(x) = x * sigmoid(x) = SiLU(x)
# âŠ™ è¡¨ç¤ºé€å…ƒç´ ç›¸ä¹˜

# GeGLU (æ›¿ä»£æ–¹æ¡ˆ)
GeGLU(x) = (GELU(W1 * x) âŠ™ W3 * x) * W2
```

### 4. GRUç­–ç•¥ç½‘ç»œ

#### é€’å½’çŠ¶æ€æ›´æ–°æœºåˆ¶
```python
# æ¢¯åº¦ä¼ æ’­è·¯å¾„
final_loss â†’ position_logits[19] â†’ strategy_state[19]
          â†’ position_logits[18] â†’ strategy_state[18]
          â†’ ... â†’ position_logits[0]

# å†…å­˜ä¼˜åŒ–
for day in range(20):
    # ä¿æŒæ¢¯åº¦çš„éƒ¨åˆ†
    position = model.predict_position(features[day], strategy_state)
    strategy_state = model.update_state(strategy_state, position)

    # ä¸ä¿æŒæ¢¯åº¦çš„éƒ¨åˆ†
    with torch.no_grad():
        portfolio_value *= (1 + position * returns[day])
```

#### å¯å¾®åˆ†ä»“ä½é¢„æµ‹
```python
class GumbelSoftmaxPositionHead(nn.Module):
    def forward(self, x, temperature=1.0, hard=False):
        logits = self.linear(x)  # [batch_size, 11] (0-10ä»“ä½)
        
        if self.training:
            # è®­ç»ƒæ—¶ï¼šGumbel-Softmaxï¼Œå¯å¾®åˆ†
            gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits)))
            y = F.softmax((logits + gumbel_noise) / temperature, dim=-1)
            
            if hard:
                # ç›´é€šä¼°è®¡å™¨
                y_hard = torch.zeros_like(y)
                y_hard.scatter_(1, y.argmax(dim=1, keepdim=True), 1.0)
                y = (y_hard - y).detach() + y
                
            # è®¡ç®—æœŸæœ›ä»“ä½
            position_values = torch.arange(11, device=x.device, dtype=torch.float32)
            positions = torch.sum(y * position_values, dim=-1, keepdim=True)
        else:
            # æ¨ç†æ—¶ï¼šç›´æ¥argmaxï¼Œç¦»æ•£
            positions = logits.argmax(dim=-1, keepdim=True).float()
            
        return {
            'logits': logits,
            'positions': positions,
            'probabilities': F.softmax(logits, dim=-1)
        }
```

## ğŸ“Š æŸå¤±å‡½æ•°è®¾è®¡

### 1. ä»·æ ¼é¢„æµ‹æŸå¤±
```python
class PricePredictionLoss(nn.Module):
    def __init__(self, loss_type='mse'):
        self.loss_type = loss_type
        
    def forward(self, predictions, targets):
        if self.loss_type == 'mse':
            return F.mse_loss(predictions, targets)
        elif self.loss_type == 'mae':
            return F.l1_loss(predictions, targets)
        elif self.loss_type == 'huber':
            return F.huber_loss(predictions, targets)
```

### 2. ç­–ç•¥æŸå¤±å‡½æ•°
```python
class StrategyLoss(nn.Module):
    def forward(self, position_predictions, next_day_returns):
        total_loss = 0
        
        for b in range(batch_size):
            positions = position_predictions[b, :, 0]  # [seq_len]
            returns = next_day_returns[b, :]           # [seq_len]
            
            # 1. åˆ¤æ–­å¸‚åœºçŠ¶æ€
            market_type = self.market_classifier.classify_market(returns)
            
            # 2. è®¡ç®—ç›¸å¯¹åŸºå‡†æ”¶ç›Š
            relative_return_loss = self._calculate_relative_return_loss(
                positions, returns, market_type
            )
            
            # 3. è®¡ç®—é£é™©æˆæœ¬
            risk_cost = self._calculate_risk_cost(positions, returns)
            
            # 4. è®¡ç®—æœºä¼šæˆæœ¬
            opportunity_cost = self._calculate_opportunity_cost(
                positions, returns, market_type
            )
            
            # 5. ç»¼åˆæŸå¤±
            sample_loss = (
                self.relative_return_weight * relative_return_loss +
                self.risk_cost_weight * risk_cost +
                self.opportunity_cost_weight * opportunity_cost
            )
            
            total_loss += sample_loss
            
        return total_loss / batch_size
```

### 3. å¸‚åœºåˆ†ç±»ç®—æ³•
```python
def classify_market(self, returns):
    # 1. ç»Ÿè®¡ç‰¹å¾
    mean_return = torch.mean(returns)
    volatility = torch.std(returns)

    # 2. æŠ€æœ¯æŒ‡æ ‡
    ma_short = torch.mean(returns[-5:])
    ma_long = torch.mean(returns[-10:])

    # 3. æŠ•ç¥¨æœºåˆ¶
    votes = [
        self._simple_classifier(mean_return),
        self._technical_classifier(ma_short, ma_long),
        self._adaptive_classifier(returns)
    ]

    return self._majority_vote(votes)
```

## ğŸ¯ æ€§èƒ½ä¼˜åŒ–

### è®­ç»ƒæ€§èƒ½å¯¹æ¯”
| é…ç½® | å‚æ•°é‡ | è®­ç»ƒé€Ÿåº¦ | å†…å­˜ä½¿ç”¨ | æ¨èåœºæ™¯ |
|------|--------|----------|----------|----------|
| Tiny | 2.5M | å¿« | 2GB | å¿«é€Ÿå®éªŒ |
| Small | 10M | ä¸­ç­‰ | 4GB | ä¸ªäººå¼€å‘ |
| Base | 40M | æ…¢ | 8GB | æœåŠ¡å™¨è®­ç»ƒ |
| Large | 160M | å¾ˆæ…¢ | 16GB | ç”Ÿäº§ç¯å¢ƒ |

### å†…å­˜ä¼˜åŒ–æŠ€å·§
```python
# 1. æ¢¯åº¦ç´¯ç§¯
accumulation_steps = 4
for i, batch in enumerate(dataloader):
    loss = model(batch) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()

# 2. æ··åˆç²¾åº¦è®­ç»ƒ
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
with autocast():
    outputs = model(inputs)
    loss = criterion(outputs, targets)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**Q: è®­ç»ƒæ—¶å†…å­˜ä¸è¶³ï¼Ÿ**
```python
# è§£å†³æ–¹æ¡ˆï¼šå‡å°æ‰¹æ¬¡å¤§å°
config.batch_size = 1  # ä»é»˜è®¤çš„4å‡å°‘åˆ°1
config.strategy_state_dim = 64  # å‡å°çŠ¶æ€ç»´åº¦
```

**Q: æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸ï¼Ÿ**
```python
# è§£å†³æ–¹æ¡ˆï¼šè°ƒæ•´å­¦ä¹ ç‡å’Œæ¢¯åº¦è£å‰ª
optimizer = optim.AdamW(model.parameters(), lr=1e-4)  # é™ä½å­¦ä¹ ç‡
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # æ¢¯åº¦è£å‰ª
```

**Q: æ¨¡å‹ä¸æ”¶æ•›ï¼Ÿ**
```python
# è§£å†³æ–¹æ¡ˆï¼šæ£€æŸ¥æ•°æ®å’ŒæŸå¤±æƒé‡
config.information_ratio_weight = 0.5  # é™ä½ä¿¡æ¯æ¯”ç‡æƒé‡
config.opportunity_cost_weight = 0.05   # é™ä½æœºä¼šæˆæœ¬æƒé‡
```

### è°ƒè¯•æŠ€å·§
```python
# 1. æ£€æŸ¥æ¢¯åº¦æµ
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"{name}: {param.grad.norm().item():.6f}")

# 2. ç›‘æ§çŠ¶æ€å˜åŒ–
print(f"çŠ¶æ€å˜åŒ–: {torch.mean(torch.abs(new_state - old_state)).item():.6f}")

# 3. éªŒè¯å¸‚åœºåˆ†ç±»
market_type = model.market_classifier.classify_market(returns)
print(f"å¸‚åœºç±»å‹: {market_type}")
```

## ğŸ“ˆ æ‰©å±•åŠŸèƒ½

### 1. ä¸“å®¶æ··åˆç½‘ç»œ (MoE)
```python
class MoEFFN(nn.Module):
    def __init__(self, args, num_experts=8, top_k=2):
        self.gate = nn.Linear(args.d_model, num_experts, bias=False)
        self.experts = nn.ModuleList([
            SwiGLU(args) for _ in range(num_experts)
        ])
```

### 2. å¤šèµ„äº§ç»„åˆä¼˜åŒ–
```python
# æœªæ¥åŠŸèƒ½ï¼šæ”¯æŒå¤šåªè‚¡ç¥¨çš„ç»„åˆä¼˜åŒ–
class PortfolioOptimizer(nn.Module):
    def __init__(self, num_assets, d_model):
        self.asset_encoders = nn.ModuleList([
            PriceTransformer(config) for _ in range(num_assets)
        ])
        self.portfolio_head = nn.Linear(d_model * num_assets, num_assets)
```

### 3. å¼ºåŒ–å­¦ä¹ é›†æˆ
```python
# æœªæ¥åŠŸèƒ½ï¼šé›†æˆPPO/SACç­‰å¼ºåŒ–å­¦ä¹ ç®—æ³•
class RLStrategyNetwork(nn.Module):
    def __init__(self, config):
        self.actor = GRUStrategyNetwork(config)
        self.critic = nn.Linear(config.d_model, 1)
```

## ğŸ”® æœªæ¥è§„åˆ’

### å³å°†æ¨å‡ºçš„åŠŸèƒ½
- [ ] **å¤šèµ„äº§ç»„åˆ**: æ”¯æŒå¤šåªè‚¡ç¥¨çš„ç»„åˆä¼˜åŒ–
- [ ] **å¼ºåŒ–å­¦ä¹ **: é›†æˆPPO/SACç­‰å¼ºåŒ–å­¦ä¹ ç®—æ³•
- [ ] **å®æ—¶äº¤æ˜“**: å¯¹æ¥å®ç›˜äº¤æ˜“æ¥å£
- [ ] **é£é™©æ¨¡å‹**: æ›´ç²¾ç»†çš„é£é™©æ§åˆ¶æ¨¡å—
- [ ] **å› å­æŒ–æ˜**: è‡ªåŠ¨ç‰¹å¾å·¥ç¨‹å’Œå› å­å‘ç°

### æŠ€æœ¯æ”¹è¿›
- [ ] **æ¨¡å‹å‹ç¼©**: çŸ¥è¯†è’¸é¦å’Œæ¨¡å‹å‰ªæ
- [ ] **åˆ†å¸ƒå¼è®­ç»ƒ**: å¤šGPUå’Œå¤šæœºè®­ç»ƒæ”¯æŒ
- [ ] **åœ¨çº¿å­¦ä¹ **: å¢é‡å­¦ä¹ å’Œæ¨¡å‹æ›´æ–°
- [ ] **è§£é‡Šæ€§**: æ³¨æ„åŠ›å¯è§†åŒ–å’Œå†³ç­–è§£é‡Š

---

**ğŸ“– æœ¬æ–‡æ¡£æŒç»­æ›´æ–°ä¸­ï¼Œæ¬¢è¿æå‡ºå»ºè®®å’Œé—®é¢˜ï¼**
