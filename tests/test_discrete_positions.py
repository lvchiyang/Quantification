#!/usr/bin/env python3
"""
æµ‹è¯•ç¦»æ•£ä»“ä½é¢„æµ‹æ–¹æ³•
"""

import torch
import torch.nn as nn
import numpy as np
import sys
import os

# æ·»åŠ  src ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from config import ModelConfigs
from transformer import FinancialTransformer
from discrete_position_methods import (
    GumbelSoftmaxPositionHead, 
    StraightThroughPositionHead, 
    ConcretePositionHead,
    DiscretePositionLoss
)


def test_position_heads():
    """æµ‹è¯•ä¸åŒçš„ä»“ä½é¢„æµ‹å¤´"""
    print("ğŸ”§ æµ‹è¯•ç¦»æ•£ä»“ä½é¢„æµ‹å¤´...")
    
    d_model = 256
    batch_size = 8
    hidden_states = torch.randn(batch_size, d_model)
    next_day_returns = torch.randn(batch_size) * 0.05  # æ¨¡æ‹ŸÂ±5%çš„æ¶¨è·Œå¹…
    
    methods = {
        'Gumbel-Softmax': GumbelSoftmaxPositionHead(d_model),
        'Straight-Through': StraightThroughPositionHead(d_model),
        'Concrete': ConcretePositionHead(d_model)
    }
    
    loss_fn = DiscretePositionLoss(max_position=10)
    
    print(f"ğŸ“Š æµ‹è¯•æ•°æ®: batch_size={batch_size}, d_model={d_model}")
    print(f"ğŸ“ˆ æ¨¡æ‹Ÿæ¶¨è·Œå¹…èŒƒå›´: {next_day_returns.min().item():.4f} ~ {next_day_returns.max().item():.4f}")
    
    for name, head in methods.items():
        print(f"\nğŸ” æµ‹è¯• {name} æ–¹æ³•:")
        
        # å‰å‘ä¼ æ’­
        head.train()
        output_train = head(hidden_states)
        
        head.eval()
        output_eval = head(hidden_states)
        
        # æ£€æŸ¥è¾“å‡º
        positions_train = output_train['positions']
        positions_eval = output_eval['positions']
        
        print(f"  è®­ç»ƒæ¨¡å¼ä»“ä½èŒƒå›´: {positions_train.min().item():.2f} ~ {positions_train.max().item():.2f}")
        print(f"  æ¨ç†æ¨¡å¼ä»“ä½èŒƒå›´: {positions_eval.min().item():.2f} ~ {positions_eval.max().item():.2f}")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ•´æ•°ï¼ˆå¯¹äºæŸäº›æ–¹æ³•ï¼‰
        if 'discrete_positions' in output_eval:
            discrete_pos = output_eval['discrete_positions']
            is_integer = torch.allclose(discrete_pos, torch.round(discrete_pos))
            print(f"  ç¦»æ•£ä»“ä½æ˜¯å¦ä¸ºæ•´æ•°: {is_integer}")
            print(f"  ç¦»æ•£ä»“ä½ç¤ºä¾‹: {discrete_pos[:5].flatten().tolist()}")
        
        # æµ‹è¯•æ¢¯åº¦
        head.train()
        output_grad = head(hidden_states)
        loss = loss_fn(output_grad, next_day_returns)
        loss.backward()
        
        # æ£€æŸ¥æ¢¯åº¦
        has_grad = any(p.grad is not None and p.grad.abs().sum() > 0 for p in head.parameters())
        print(f"  æ¢¯åº¦è®¡ç®—: {'âœ… æ­£å¸¸' if has_grad else 'âŒ å¼‚å¸¸'}")
        print(f"  æŸå¤±å€¼: {loss.item():.6f}")
        
        # æ¸…é™¤æ¢¯åº¦
        head.zero_grad()


def test_model_integration():
    """æµ‹è¯•æ¨¡å‹é›†æˆ"""
    print("\nğŸ”§ æµ‹è¯•æ¨¡å‹é›†æˆ...")
    
    methods = ['gumbel_softmax', 'straight_through', 'concrete']
    
    for method in methods:
        print(f"\nğŸ” æµ‹è¯• {method} æ–¹æ³•é›†æˆ:")
        
        # åˆ›å»ºé…ç½®
        config = ModelConfigs.tiny()
        config.position_method = method
        
        # åˆ›å»ºæ¨¡å‹
        model = FinancialTransformer(config)
        model.train()
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size = 4
        seq_len = 180
        n_features = 11
        
        financial_data = torch.randn(batch_size, seq_len, n_features)
        target_prices = torch.randn(batch_size, 7)
        next_day_returns = torch.randn(batch_size) * 0.03
        
        # å‰å‘ä¼ æ’­
        outputs = model(
            financial_data=financial_data,
            target_prices=target_prices,
            next_day_returns=next_day_returns,
            return_dict=True
        )
        
        # æ£€æŸ¥è¾“å‡º
        print(f"  ä»·æ ¼é¢„æµ‹å½¢çŠ¶: {outputs['price_predictions'].shape}")
        print(f"  ä»“ä½é¢„æµ‹å½¢çŠ¶: {outputs['position_predictions'].shape}")
        print(f"  ä»“ä½èŒƒå›´: {outputs['position_predictions'].min().item():.2f} ~ {outputs['position_predictions'].max().item():.2f}")
        
        # æ£€æŸ¥æŸå¤±
        total_loss = outputs['loss']
        price_loss = outputs.get('price_loss', torch.tensor(0.0))
        position_loss = outputs.get('position_loss', torch.tensor(0.0))
        
        print(f"  æ€»æŸå¤±: {total_loss.item():.6f}")
        print(f"  ä»·æ ¼æŸå¤±: {price_loss.item():.6f}")
        print(f"  ä»“ä½æŸå¤±: {position_loss.item():.6f}")
        
        # æµ‹è¯•åå‘ä¼ æ’­
        total_loss.backward()
        
        # æ£€æŸ¥æ¢¯åº¦
        has_grad = any(p.grad is not None and p.grad.abs().sum() > 0 for p in model.parameters())
        print(f"  æ¢¯åº¦è®¡ç®—: {'âœ… æ­£å¸¸' if has_grad else 'âŒ å¼‚å¸¸'}")
        
        # æ£€æŸ¥ä»“ä½é¢„æµ‹å¤´çš„æ¢¯åº¦
        if hasattr(model.position_head, 'head'):
            pos_head_grad = any(p.grad is not None and p.grad.abs().sum() > 0 for p in model.position_head.head.parameters())
            print(f"  ä»“ä½å¤´æ¢¯åº¦: {'âœ… æ­£å¸¸' if pos_head_grad else 'âŒ å¼‚å¸¸'}")


def test_discrete_output():
    """æµ‹è¯•ç¦»æ•£è¾“å‡ºçš„æ•´æ•°æ€§è´¨"""
    print("\nğŸ”§ æµ‹è¯•ç¦»æ•£è¾“å‡º...")
    
    d_model = 256
    batch_size = 100  # ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡æµ‹è¯•
    
    # æµ‹è¯•Gumbel-Softmaxåœ¨ä¸åŒæ¸©åº¦ä¸‹çš„è¡¨ç°
    head = GumbelSoftmaxPositionHead(d_model)
    hidden_states = torch.randn(batch_size, d_model)
    
    temperatures = [0.1, 0.5, 1.0, 2.0, 5.0]
    
    print("ğŸ“Š Gumbel-Softmaxåœ¨ä¸åŒæ¸©åº¦ä¸‹çš„ç¦»æ•£ç¨‹åº¦:")
    
    for temp in temperatures:
        head.temperature.data = torch.tensor(temp)
        head.eval()  # æ¨ç†æ¨¡å¼ä½¿ç”¨ç¡¬é‡‡æ ·
        
        output = head(hidden_states)
        positions = output['positions']
        discrete_positions = output.get('discrete_positions', positions)
        
        # è®¡ç®—ç¦»æ•£ç¨‹åº¦ï¼ˆä¸æœ€è¿‘æ•´æ•°çš„å¹³å‡è·ç¦»ï¼‰
        discreteness = torch.mean(torch.abs(positions - torch.round(positions))).item()
        
        # è®¡ç®—ä»“ä½åˆ†å¸ƒ
        rounded_positions = torch.round(discrete_positions).long()
        position_counts = torch.bincount(rounded_positions.flatten(), minlength=11)
        position_probs = position_counts.float() / batch_size
        
        print(f"  æ¸©åº¦ {temp:3.1f}: ç¦»æ•£åº¦={discreteness:.4f}, åˆ†å¸ƒç†µ={-torch.sum(position_probs * torch.log(position_probs + 1e-8)).item():.3f}")
        
        # æ˜¾ç¤ºå‰10ä¸ªä½ç½®çš„åˆ†å¸ƒ
        top_positions = position_probs[:10].tolist()
        print(f"           ä»“ä½0-9åˆ†å¸ƒ: {[f'{p:.2f}' for p in top_positions]}")


def test_gradient_flow():
    """æµ‹è¯•æ¢¯åº¦æµåŠ¨"""
    print("\nğŸ”§ æµ‹è¯•æ¢¯åº¦æµåŠ¨...")
    
    d_model = 256
    batch_size = 16
    
    methods = {
        'Gumbel-Softmax': GumbelSoftmaxPositionHead(d_model),
        'Straight-Through': StraightThroughPositionHead(d_model),
        'Concrete': ConcretePositionHead(d_model)
    }
    
    hidden_states = torch.randn(batch_size, d_model, requires_grad=True)
    next_day_returns = torch.randn(batch_size) * 0.05
    loss_fn = DiscretePositionLoss(max_position=10)
    
    print("ğŸ“Š ä¸åŒæ–¹æ³•çš„æ¢¯åº¦ç»Ÿè®¡:")
    
    for name, head in methods.items():
        head.train()
        
        # å‰å‘ä¼ æ’­
        output = head(hidden_states)
        loss = loss_fn(output, next_day_returns)
        
        # åå‘ä¼ æ’­
        loss.backward(retain_graph=True)
        
        # ç»Ÿè®¡æ¢¯åº¦
        input_grad_norm = hidden_states.grad.norm().item() if hidden_states.grad is not None else 0.0
        param_grad_norms = [p.grad.norm().item() for p in head.parameters() if p.grad is not None]
        avg_param_grad = np.mean(param_grad_norms) if param_grad_norms else 0.0
        
        print(f"  {name:15s}: è¾“å…¥æ¢¯åº¦èŒƒæ•°={input_grad_norm:.6f}, å¹³å‡å‚æ•°æ¢¯åº¦èŒƒæ•°={avg_param_grad:.6f}")
        
        # æ¸…é™¤æ¢¯åº¦
        hidden_states.grad = None
        head.zero_grad()


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•ç¦»æ•£ä»“ä½é¢„æµ‹æ–¹æ³•...\n")
    
    try:
        test_position_heads()
        test_model_integration()
        test_discrete_output()
        test_gradient_flow()
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("\nğŸ“‹ æ–¹æ³•æ€»ç»“:")
        print("âœ… Gumbel-Softmax: ç†è®ºæœ€ä¼˜ï¼Œæ¸©åº¦å¯è°ƒï¼Œæ”¯æŒç¡¬/è½¯é‡‡æ ·")
        print("âœ… Straight-Through: ç®€å•ç›´æ¥ï¼Œå‰å‘ç¦»æ•£ï¼Œåå‘è¿ç»­")
        print("âœ… Concrete: æ— å™ªå£°ï¼Œæ¸©åº¦æ§åˆ¶ï¼Œè®­ç»ƒç¨³å®š")
        print("\nğŸš€ æ¨èä½¿ç”¨ Gumbel-Softmax æ–¹æ³•è¿›è¡Œè®­ç»ƒï¼")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
