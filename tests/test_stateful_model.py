#!/usr/bin/env python3
"""
æµ‹è¯•çŠ¶æ€åŒ–æ¨¡å‹åŠŸèƒ½
"""

import torch
import numpy as np
import sys
import os

# æ·»åŠ  src ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from config import ModelConfigs
from transformer import FinancialTransformer
from market_classifier import ComprehensiveMarketClassifier
from information_ratio_loss import InformationRatioLoss
from recurrent_trainer import RecurrentStrategyTrainer


def test_market_classifier():
    """æµ‹è¯•å¸‚åœºåˆ†ç±»å™¨"""
    print("ğŸ”§ æµ‹è¯•å¸‚åœºåˆ†ç±»å™¨...")
    
    classifier = ComprehensiveMarketClassifier()
    
    # æµ‹è¯•ä¸åŒå¸‚åœºç¯å¢ƒ
    test_cases = [
        (torch.tensor([0.02, 0.015, 0.01, 0.025, 0.018]), "ç‰›å¸‚"),
        (torch.tensor([-0.02, -0.015, -0.01, -0.025, -0.018]), "ç†Šå¸‚"),
        (torch.tensor([0.005, -0.003, 0.002, -0.001, 0.004]), "éœ‡è¡å¸‚")
    ]
    
    for returns, expected in test_cases:
        market_type = classifier.classify_market(returns)
        benchmark = classifier.get_optimal_benchmark(market_type)
        
        print(f"  {expected}: åˆ†ç±»={market_type}, åŸºå‡†={benchmark['name']}")
    
    print("âœ… å¸‚åœºåˆ†ç±»å™¨æµ‹è¯•é€šè¿‡")


def test_information_ratio_loss():
    """æµ‹è¯•ä¿¡æ¯æ¯”ç‡æŸå¤±å‡½æ•°"""
    print("ğŸ”§ æµ‹è¯•ä¿¡æ¯æ¯”ç‡æŸå¤±å‡½æ•°...")
    
    classifier = ComprehensiveMarketClassifier()
    loss_fn = InformationRatioLoss(classifier)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 3
    seq_len = 20
    
    position_predictions = torch.rand(batch_size, seq_len, 1) * 10  # [0, 10]
    next_day_returns = torch.randn(batch_size, seq_len) * 0.02      # Â±2%
    
    # è®¡ç®—æŸå¤±
    loss_dict = loss_fn(position_predictions, next_day_returns)
    
    print(f"  ä¿¡æ¯æ¯”ç‡æŸå¤±: {loss_dict['total_loss']:.6f}")
    print(f"  å¹³å‡ä¿¡æ¯æ¯”ç‡: {loss_dict['information_ratio']:.4f}")
    print(f"  æœºä¼šæˆæœ¬: {loss_dict['opportunity_cost']:.6f}")
    print(f"  é£é™©æƒ©ç½š: {loss_dict['risk_penalty']:.6f}")
    
    print("âœ… ä¿¡æ¯æ¯”ç‡æŸå¤±æµ‹è¯•é€šè¿‡")


def test_stateful_model():
    """æµ‹è¯•çŠ¶æ€åŒ–æ¨¡å‹"""
    print("ğŸ”§ æµ‹è¯•çŠ¶æ€åŒ–æ¨¡å‹...")
    
    # åˆ›å»ºé…ç½®
    config = ModelConfigs.tiny()
    config.enable_stateful_training = True
    config.strategy_state_dim = 64
    config.state_update_method = 'gru'
    
    # åˆ›å»ºæ¨¡å‹
    model = FinancialTransformer(config)
    
    # æµ‹è¯•å•æ—¥é¢„æµ‹
    batch_size = 2
    seq_len = 180
    n_features = 11
    
    financial_data = torch.randn(batch_size, seq_len, n_features)
    
    # ç¬¬ä¸€æ¬¡é¢„æµ‹ï¼ˆæ— çŠ¶æ€ï¼‰
    outputs1 = model.forward_single_day(financial_data, return_dict=True)
    
    print(f"  ç¬¬ä¸€æ¬¡é¢„æµ‹:")
    print(f"    ä»·æ ¼é¢„æµ‹å½¢çŠ¶: {outputs1['price_predictions'].shape}")
    print(f"    ä»“ä½é¢„æµ‹å½¢çŠ¶: {outputs1['position_predictions'].shape}")
    print(f"    ç­–ç•¥çŠ¶æ€å½¢çŠ¶: {outputs1['strategy_state'].shape}")
    
    # ç¬¬äºŒæ¬¡é¢„æµ‹ï¼ˆä½¿ç”¨çŠ¶æ€ï¼‰
    outputs2 = model.forward_single_day(
        financial_data, 
        strategy_state=outputs1['strategy_state'],
        return_dict=True
    )
    
    print(f"  ç¬¬äºŒæ¬¡é¢„æµ‹:")
    print(f"    ä»“ä½é¢„æµ‹å˜åŒ–: {torch.mean(torch.abs(outputs2['position_predictions'] - outputs1['position_predictions'])).item():.4f}")
    print(f"    çŠ¶æ€å˜åŒ–: {torch.mean(torch.abs(outputs2['strategy_state'] - outputs1['strategy_state'])).item():.4f}")
    
    print("âœ… çŠ¶æ€åŒ–æ¨¡å‹æµ‹è¯•é€šè¿‡")


def test_recurrent_trainer():
    """æµ‹è¯•é€’å½’è®­ç»ƒå™¨"""
    print("ğŸ”§ æµ‹è¯•é€’å½’è®­ç»ƒå™¨...")
    
    # åˆ›å»ºæ¨¡å‹
    config = ModelConfigs.tiny()
    config.enable_stateful_training = True
    config.strategy_state_dim = 32
    
    model = FinancialTransformer(config)
    trainer = RecurrentStrategyTrainer(model, use_information_ratio=True)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 2
    n_slides = 20
    seq_len = 180
    n_features = 11
    
    sliding_window_data = {
        'features': torch.randn(batch_size, n_slides, seq_len, n_features),
        'price_targets': torch.randn(batch_size, n_slides, 7),
        'next_day_returns': torch.randn(batch_size, n_slides) * 0.02
    }
    
    # è®­ç»ƒæ­¥éª¤
    model.train()
    loss_dict = trainer.train_step(sliding_window_data)
    
    print(f"  è®­ç»ƒæŸå¤±: {loss_dict['total_loss']:.6f}")
    print(f"  ä»·æ ¼æŸå¤±: {loss_dict['price_loss']:.6f}")
    print(f"  ä¿¡æ¯æ¯”ç‡: {loss_dict['information_ratio']:.4f}")
    print(f"  ç´¯è®¡æ”¶ç›Š: {loss_dict['mean_cumulative_return']:+.4f}")
    print(f"  å¤æ™®æ¯”ç‡: {loss_dict['sharpe_ratio']:.4f}")
    
    # éªŒè¯æ­¥éª¤
    val_dict = trainer.validate_step(sliding_window_data)
    
    print(f"  éªŒè¯æŸå¤±: {val_dict['total_loss']:.6f}")
    print(f"  éªŒè¯æ”¶ç›Š: {val_dict['mean_cumulative_return']:+.4f}")
    
    # æµ‹è¯•æ¢¯åº¦
    loss_tensor = loss_dict['loss_tensor']
    loss_tensor.backward()
    
    # æ£€æŸ¥æ¢¯åº¦
    has_grad = any(p.grad is not None and p.grad.abs().sum() > 0 for p in model.parameters())
    print(f"  æ¢¯åº¦è®¡ç®—: {'âœ… æ­£å¸¸' if has_grad else 'âŒ å¼‚å¸¸'}")
    
    print("âœ… é€’å½’è®­ç»ƒå™¨æµ‹è¯•é€šè¿‡")


def test_memory_efficiency():
    """æµ‹è¯•å†…å­˜æ•ˆç‡"""
    print("ğŸ”§ æµ‹è¯•å†…å­˜æ•ˆç‡...")
    
    import psutil
    import gc
    
    # è·å–åˆå§‹å†…å­˜
    process = psutil.Process()
    initial_memory = process.memory_info().rss / 1024 / 1024  # MB
    
    # åˆ›å»ºæ¨¡å‹
    config = ModelConfigs.tiny()
    config.enable_stateful_training = True
    
    model = FinancialTransformer(config)
    trainer = RecurrentStrategyTrainer(model)
    
    # åˆ›å»ºè¾ƒå¤§çš„æµ‹è¯•æ•°æ®
    batch_size = 4
    sliding_window_data = {
        'features': torch.randn(batch_size, 20, 180, 11),
        'price_targets': torch.randn(batch_size, 20, 7),
        'next_day_returns': torch.randn(batch_size, 20) * 0.02
    }
    
    # æ‰§è¡Œè®­ç»ƒæ­¥éª¤
    model.train()
    loss_dict = trainer.train_step(sliding_window_data)
    loss_dict['loss_tensor'].backward()
    
    # è·å–å³°å€¼å†…å­˜
    peak_memory = process.memory_info().rss / 1024 / 1024  # MB
    memory_increase = peak_memory - initial_memory
    
    print(f"  åˆå§‹å†…å­˜: {initial_memory:.1f} MB")
    print(f"  å³°å€¼å†…å­˜: {peak_memory:.1f} MB")
    print(f"  å†…å­˜å¢é•¿: {memory_increase:.1f} MB")
    
    # æ¸…ç†
    del model, trainer, sliding_window_data, loss_dict
    gc.collect()
    torch.cuda.empty_cache() if torch.cuda.is_available() else None
    
    print("âœ… å†…å­˜æ•ˆç‡æµ‹è¯•å®Œæˆ")


def test_gradient_flow():
    """æµ‹è¯•æ¢¯åº¦æµåŠ¨"""
    print("ğŸ”§ æµ‹è¯•æ¢¯åº¦æµåŠ¨...")
    
    config = ModelConfigs.tiny()
    config.enable_stateful_training = True
    config.strategy_state_dim = 32
    
    model = FinancialTransformer(config)
    
    # åˆ›å»ºéœ€è¦æ¢¯åº¦çš„è¾“å…¥
    financial_data = torch.randn(2, 180, 11, requires_grad=True)
    
    # å¤šæ­¥é¢„æµ‹
    strategy_state = None
    total_loss = 0.0
    
    for step in range(5):  # 5æ­¥é¢„æµ‹
        outputs = model.forward_single_day(financial_data, strategy_state, return_dict=True)
        
        # ç®€å•æŸå¤±
        loss = torch.mean(outputs['position_predictions'] ** 2)
        total_loss += loss
        
        strategy_state = outputs['strategy_state']
    
    # åå‘ä¼ æ’­
    total_loss.backward()
    
    # æ£€æŸ¥æ¢¯åº¦
    input_grad_norm = financial_data.grad.norm().item() if financial_data.grad is not None else 0.0
    
    param_grad_norms = []
    for name, param in model.named_parameters():
        if param.grad is not None:
            param_grad_norms.append(param.grad.norm().item())
    
    avg_param_grad = np.mean(param_grad_norms) if param_grad_norms else 0.0
    
    print(f"  è¾“å…¥æ¢¯åº¦èŒƒæ•°: {input_grad_norm:.6f}")
    print(f"  å¹³å‡å‚æ•°æ¢¯åº¦èŒƒæ•°: {avg_param_grad:.6f}")
    print(f"  æœ‰æ¢¯åº¦çš„å‚æ•°æ•°é‡: {len(param_grad_norms)}")
    
    print("âœ… æ¢¯åº¦æµåŠ¨æµ‹è¯•é€šè¿‡")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•çŠ¶æ€åŒ–æ¨¡å‹åŠŸèƒ½...\n")
    
    try:
        test_market_classifier()
        print()
        
        test_information_ratio_loss()
        print()
        
        test_stateful_model()
        print()
        
        test_recurrent_trainer()
        print()
        
        test_memory_efficiency()
        print()
        
        test_gradient_flow()
        print()
        
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("\nğŸ“‹ åŠŸèƒ½æ€»ç»“:")
        print("âœ… å¸‚åœºçŠ¶æ€è‡ªåŠ¨åˆ†ç±»ï¼ˆç‰›å¸‚/ç†Šå¸‚/éœ‡è¡å¸‚ï¼‰")
        print("âœ… ä¿¡æ¯æ¯”ç‡æŸå¤±å‡½æ•°ï¼ˆè‡ªé€‚åº”åŸºå‡†æ¯”è¾ƒï¼‰")
        print("âœ… é€’å½’çŠ¶æ€æ›´æ–°ï¼ˆ20å¤©è®°å¿†ä¿æŒï¼‰")
        print("âœ… å†…å­˜é«˜æ•ˆè®­ç»ƒï¼ˆé¿å…20å€å†…å­˜å¼€é”€ï¼‰")
        print("âœ… æ¢¯åº¦æ­£å¸¸ä¼ æ’­ï¼ˆæ‰€æœ‰å‚æ•°å¯è®­ç»ƒï¼‰")
        print("âœ… æœºä¼šæˆæœ¬è®¡ç®—ï¼ˆè§£å†³è¿ç»­ä¸Šæ¶¨è¯„ä»·é—®é¢˜ï¼‰")
        
        print("\nğŸš€ å¯ä»¥å¼€å§‹è®­ç»ƒçŠ¶æ€åŒ–æ¨¡å‹äº†ï¼")
        print("è¿è¡Œ: python train_stateful_strategy.py")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
