#!/usr/bin/env python3
"""
çŠ¶æ€åŒ–äº¤æ˜“ç­–ç•¥è®­ç»ƒè„šæœ¬
åŸºäºé€’å½’çŠ¶æ€æ›´æ–°å’Œä¿¡æ¯æ¯”ç‡æŸå¤±çš„è®­ç»ƒ
"""

import torch
import torch.optim as optim
import numpy as np
import sys
import os
from typing import Dict, Any

# æ·»åŠ  src ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from config import ModelConfigs
from transformer import FinancialTransformer
from financial_data import FinancialDataProcessor, create_sample_data
from recurrent_trainer import RecurrentStrategyTrainer, create_sliding_window_batches


def create_stateful_dataset():
    """åˆ›å»ºçŠ¶æ€åŒ–è®­ç»ƒæ•°æ®é›†"""
    print("ğŸ”„ åˆ›å»ºçŠ¶æ€åŒ–è®­ç»ƒæ•°æ®...")
    
    # ç”Ÿæˆæ›´é•¿çš„ç¤ºä¾‹æ•°æ®
    sample_data = create_sample_data(n_days=600)  # æ›´å¤šæ•°æ®ç”¨äºè®­ç»ƒ
    
    # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
    temp_file = "temp_stateful_data.txt"
    with open(temp_file, 'w', encoding='utf-8') as f:
        f.write(sample_data)
    
    # å¤„ç†æ•°æ®
    processor = FinancialDataProcessor(
        sequence_length=180,
        prediction_horizon=7,
        trading_horizon=20,
        normalize=True,
        enable_trading_strategy=True,
        sliding_window=True
    )
    
    # è¯»å–å¹¶å¤„ç†æ•°æ®
    with open(temp_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    data_list = []
    for line in lines:
        if line.strip():
            parsed = processor.parse_data_line(line)
            if parsed:
                data_list.append(parsed)
    
    if not data_list:
        raise ValueError("æ²¡æœ‰æˆåŠŸè§£æä»»ä½•æ•°æ®")
    
    # è½¬æ¢ä¸ºDataFrame
    import pandas as pd
    df = pd.DataFrame(data_list)
    df = processor.add_time_encoding(df)
    processor.fit_normalizer(df)
    df = processor.normalize_features(df)
    
    # åˆ›å»ºæ»‘åŠ¨çª—å£åºåˆ—
    features_list, price_targets_list, _, next_day_returns = processor.create_sliding_window_sequences(df)
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    os.remove(temp_file)
    
    return features_list, price_targets_list, next_day_returns, processor


def train_stateful_model():
    """è®­ç»ƒçŠ¶æ€åŒ–æ¨¡å‹"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒçŠ¶æ€åŒ–äº¤æ˜“ç­–ç•¥æ¨¡å‹...")
    
    # 1. åˆ›å»ºæ•°æ®
    features_list, price_targets_list, next_day_returns, processor = create_stateful_dataset()
    
    print(f"ğŸ“Š æ•°æ®å½¢çŠ¶:")
    print(f"  - ç‰¹å¾åºåˆ—: {features_list.shape}")      # [n_sequences, 20, 180, 11]
    print(f"  - ä»·æ ¼ç›®æ ‡: {price_targets_list.shape}")  # [n_sequences, 20, 7]
    print(f"  - æ¬¡æ—¥æ”¶ç›Š: {next_day_returns.shape}")    # [n_sequences, 20]
    
    # 2. æ•°æ®åˆ†å‰²
    n_sequences = features_list.shape[0]
    train_size = int(0.8 * n_sequences)
    
    train_features = features_list[:train_size]
    train_price_targets = price_targets_list[:train_size]
    train_returns = next_day_returns[:train_size]
    
    val_features = features_list[train_size:]
    val_price_targets = price_targets_list[train_size:]
    val_returns = next_day_returns[train_size:]
    
    print(f"ğŸ“Š æ•°æ®åˆ†å‰²:")
    print(f"  - è®­ç»ƒåºåˆ—: {len(train_features)} ä¸ª")
    print(f"  - éªŒè¯åºåˆ—: {len(val_features)} ä¸ª")
    
    # 3. åˆ›å»ºæ‰¹æ¬¡æ•°æ®
    batch_size = 2  # ç”±äºé€’å½’è®¡ç®—ï¼Œä½¿ç”¨è¾ƒå°çš„æ‰¹æ¬¡
    train_batches = create_sliding_window_batches(
        train_features, train_price_targets, train_returns, batch_size
    )
    val_batches = create_sliding_window_batches(
        val_features, val_price_targets, val_returns, batch_size
    )
    
    print(f"ğŸ“Š æ‰¹æ¬¡ä¿¡æ¯:")
    print(f"  - è®­ç»ƒæ‰¹æ¬¡: {len(train_batches)} ä¸ª")
    print(f"  - éªŒè¯æ‰¹æ¬¡: {len(val_batches)} ä¸ª")
    print(f"  - æ¯æ‰¹æ¬¡åºåˆ—æ•°: {batch_size}")
    
    # 4. åˆ›å»ºæ¨¡å‹
    config = ModelConfigs.tiny()
    
    # å¯ç”¨çŠ¶æ€åŒ–è®­ç»ƒ
    config.enable_stateful_training = True
    config.strategy_state_dim = 128  # è¾ƒå°çš„çŠ¶æ€ç»´åº¦
    config.state_update_method = 'gru'
    config.position_method = 'gumbel_softmax'
    
    # è°ƒæ•´æŸå¤±æƒé‡
    config.information_ratio_weight = 1.0
    config.opportunity_cost_weight = 0.1
    config.risk_adjustment_weight = 0.05
    config.state_regularization_weight = 0.001
    
    model = FinancialTransformer(config)
    
    print(f"ğŸ—ï¸ æ¨¡å‹åˆ›å»ºå®Œæˆ:")
    print(f"  - å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    print(f"  - çŠ¶æ€ç»´åº¦: {config.strategy_state_dim}")
    print(f"  - çŠ¶æ€æ›´æ–°æ–¹æ³•: {config.state_update_method}")
    print(f"  - ä»“ä½æ–¹æ³•: {config.position_method}")
    
    # 5. åˆ›å»ºè®­ç»ƒå™¨
    trainer = RecurrentStrategyTrainer(model, use_information_ratio=True)
    
    # 6. è®¾ç½®ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(
        model.parameters(),
        lr=config.learning_rate * 0.3,  # é™ä½å­¦ä¹ ç‡
        weight_decay=config.weight_decay,
        betas=(config.beta1, config.beta2)
    )
    
    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=25, eta_min=1e-6
    )
    
    # 7. è®­ç»ƒå¾ªç¯
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    print("ğŸ“ˆ å¼€å§‹çŠ¶æ€åŒ–è®­ç»ƒ...")
    print("ğŸ’¡ ç‰¹ç‚¹: é€’å½’çŠ¶æ€æ›´æ–° + ä¿¡æ¯æ¯”ç‡æŸå¤± + å¸‚åœºè‡ªé€‚åº”åŸºå‡†")
    
    num_epochs = 25
    best_val_return = float('-inf')
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_metrics = {
            'total_loss': 0.0,
            'price_loss': 0.0,
            'information_ratio_loss': 0.0,
            'information_ratio': 0.0,
            'opportunity_cost': 0.0,
            'risk_penalty': 0.0,
            'state_regularization': 0.0,
            'mean_cumulative_return': 0.0,
            'sharpe_ratio': 0.0
        }
        
        for batch_idx, batch_data in enumerate(train_batches):
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            for key in batch_data:
                batch_data[key] = batch_data[key].to(device)
            
            optimizer.zero_grad()
            
            # æ‰§è¡Œé€’å½’è®­ç»ƒæ­¥éª¤
            loss_dict = trainer.train_step(batch_data)
            
            # åå‘ä¼ æ’­
            loss_tensor = loss_dict['loss_tensor']
            loss_tensor.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            # ç´¯ç§¯æŒ‡æ ‡
            for key in train_metrics:
                if key in loss_dict:
                    train_metrics[key] += loss_dict[key]
            
            # æ‰“å°è¿›åº¦
            if (batch_idx + 1) % 10 == 0:
                print(f"  Batch {batch_idx+1}/{len(train_batches)}: "
                      f"Loss={loss_dict['total_loss']:.6f}, "
                      f"Return={loss_dict['mean_cumulative_return']:+.4f}, "
                      f"IR={loss_dict['information_ratio']:.4f}")
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_metrics = {
            'total_loss': 0.0,
            'price_loss': 0.0,
            'information_ratio_loss': 0.0,
            'information_ratio': 0.0,
            'opportunity_cost': 0.0,
            'risk_penalty': 0.0,
            'state_regularization': 0.0,
            'mean_cumulative_return': 0.0,
            'sharpe_ratio': 0.0
        }
        
        for batch_data in val_batches:
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            for key in batch_data:
                batch_data[key] = batch_data[key].to(device)
            
            # éªŒè¯æ­¥éª¤
            loss_dict = trainer.validate_step(batch_data)
            
            # ç´¯ç§¯æŒ‡æ ‡
            for key in val_metrics:
                if key in loss_dict:
                    val_metrics[key] += loss_dict[key]
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        for key in train_metrics:
            train_metrics[key] /= len(train_batches)
        for key in val_metrics:
            val_metrics[key] /= len(val_batches)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_metrics['mean_cumulative_return'] > best_val_return:
            best_val_return = val_metrics['mean_cumulative_return']
            torch.save(model.state_dict(), 'best_stateful_model.pth')
        
        # æ‰“å°epochç»“æœ
        print(f"\nEpoch {epoch+1:2d}/{num_epochs}:")
        print(f"  è®­ç»ƒ - æŸå¤±: {train_metrics['total_loss']:.6f}, "
              f"ç´¯è®¡æ”¶ç›Š: {train_metrics['mean_cumulative_return']:+.4f}, "
              f"ä¿¡æ¯æ¯”ç‡: {train_metrics['information_ratio']:+.4f}")
        print(f"  éªŒè¯ - æŸå¤±: {val_metrics['total_loss']:.6f}, "
              f"ç´¯è®¡æ”¶ç›Š: {val_metrics['mean_cumulative_return']:+.4f}, "
              f"ä¿¡æ¯æ¯”ç‡: {val_metrics['information_ratio']:+.4f}")
        print(f"  å­¦ä¹ ç‡: {scheduler.get_last_lr()[0]:.2e}")
    
    print("âœ… çŠ¶æ€åŒ–è®­ç»ƒå®Œæˆ!")
    
    # 8. æµ‹è¯•æœ€ä½³æ¨¡å‹
    print("\nğŸ”® æµ‹è¯•æœ€ä½³æ¨¡å‹...")
    model.load_state_dict(torch.load('best_stateful_model.pth'))
    model.eval()
    
    # ä½¿ç”¨ä¸€ä¸ªéªŒè¯æ‰¹æ¬¡è¿›è¡Œæµ‹è¯•
    if val_batches:
        test_batch = val_batches[0]
        for key in test_batch:
            test_batch[key] = test_batch[key].to(device)
        
        loss_dict = trainer.validate_step(test_batch)
        
        print(f"ğŸ“Š æœ€ä½³æ¨¡å‹æ€§èƒ½:")
        print(f"  - ç´¯è®¡æ”¶ç›Šç‡: {loss_dict['mean_cumulative_return']:+.4f} ({loss_dict['mean_cumulative_return']*100:+.2f}%)")
        print(f"  - ä¿¡æ¯æ¯”ç‡: {loss_dict['information_ratio']:+.4f}")
        print(f"  - å¤æ™®æ¯”ç‡: {loss_dict['sharpe_ratio']:+.4f}")
        print(f"  - æœ€å¤§å›æ’¤: {loss_dict['max_drawdown']:.4f} ({loss_dict['max_drawdown']*100:.2f}%)")
        print(f"  - æœºä¼šæˆæœ¬: {loss_dict['opportunity_cost']:.6f}")
        print(f"  - é£é™©æƒ©ç½š: {loss_dict['risk_penalty']:.6f}")
        
        # æ¼”ç¤ºé€’å½’é¢„æµ‹è¿‡ç¨‹
        print(f"\nğŸ“ˆ æ¼”ç¤º20å¤©é€’å½’é¢„æµ‹è¿‡ç¨‹:")
        features = test_batch['features'][:1]  # å–ç¬¬ä¸€ä¸ªåºåˆ—
        
        model.eval()
        strategy_state = None
        positions_over_time = []
        
        with torch.no_grad():
            for slide in range(20):
                slide_features = features[:, slide, :, :]
                outputs = model.forward_single_day(
                    slide_features, strategy_state, return_dict=True
                )
                
                position = outputs['position_predictions'][0, 0].item()
                strategy_state = outputs['strategy_state']
                
                positions_over_time.append(round(position))
        
        print(f"  ä»“ä½åºåˆ—: {positions_over_time}")
        print(f"  å¹³å‡ä»“ä½: {np.mean(positions_over_time):.1f}")
        print(f"  ä»“ä½å˜åŒ–: {np.std(positions_over_time):.1f}")


if __name__ == "__main__":
    train_stateful_model()
