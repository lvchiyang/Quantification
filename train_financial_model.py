#!/usr/bin/env python3
"""
é‡‘èé‡åŒ–æ¨¡å‹è®­ç»ƒè„šæœ¬
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import sys
import os
from typing import Tuple

# æ·»åŠ  src ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from config import ModelArgs, ModelConfigs
from transformer import FinancialTransformer
from financial_data import FinancialDataProcessor, create_sample_data

def create_sample_dataset() -> Tuple[torch.Tensor, torch.Tensor]:
    """åˆ›å»ºç¤ºä¾‹æ•°æ®é›†"""
    print("ğŸ”„ åˆ›å»ºç¤ºä¾‹é‡‘èæ•°æ®...")
    
    # ç”Ÿæˆç¤ºä¾‹æ•°æ®
    sample_data = create_sample_data(n_days=300)
    
    # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
    temp_file = "temp_financial_data.txt"
    with open(temp_file, 'w', encoding='utf-8') as f:
        f.write(sample_data)
    
    # å¤„ç†æ•°æ®
    processor = FinancialDataProcessor(
        sequence_length=60,
        prediction_horizon=7,
        normalize=True
    )
    
    features, targets = processor.process_file(temp_file)
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    os.remove(temp_file)
    
    return features, targets, processor

def train_model():
    """è®­ç»ƒé‡‘èé¢„æµ‹æ¨¡å‹"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒé‡‘èé‡åŒ–æ¨¡å‹...")
    
    # 1. åˆ›å»ºæ•°æ®
    features, targets, processor = create_sample_dataset()
    
    # 2. æ•°æ®åˆ†å‰²
    train_size = int(0.8 * len(features))
    train_features = features[:train_size]
    train_targets = targets[:train_size]
    val_features = features[train_size:]
    val_targets = targets[train_size:]
    
    print(f"ğŸ“Š æ•°æ®åˆ†å‰²:")
    print(f"  - è®­ç»ƒé›†: {len(train_features)} æ ·æœ¬")
    print(f"  - éªŒè¯é›†: {len(val_features)} æ ·æœ¬")
    
    # 3. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(train_features, train_targets)
    val_dataset = TensorDataset(val_features, val_targets)
    
    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)
    
    # 4. åˆ›å»ºæ¨¡å‹
    config = ModelConfigs.tiny()  # ä½¿ç”¨å°æ¨¡å‹è¿›è¡Œæµ‹è¯•
    model = FinancialTransformer(config)
    
    print(f"ğŸ—ï¸ æ¨¡å‹åˆ›å»ºå®Œæˆ:")
    print(f"  - å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    print(f"  - è¾“å…¥ç‰¹å¾: {config.n_features}")
    print(f"  - é¢„æµ‹æ•°é‡: {config.n_predictions}")
    
    # 5. è®¾ç½®ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.AdamW(
        model.parameters(),
        lr=config.learning_rate,
        weight_decay=config.weight_decay,
        betas=(config.beta1, config.beta2)
    )
    
    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=50, eta_min=1e-6
    )
    
    # 6. è®­ç»ƒå¾ªç¯
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    print("ğŸ“ˆ å¼€å§‹è®­ç»ƒ...")
    
    num_epochs = 20
    best_val_loss = float('inf')
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_batches = 0
        
        for batch_features, batch_targets in train_loader:
            batch_features = batch_features.to(device)
            batch_targets = batch_targets.to(device)
            
            optimizer.zero_grad()
            
            outputs = model(
                financial_data=batch_features,
                target_prices=batch_targets,
                return_dict=True
            )
            
            loss = outputs['loss']
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            train_loss += loss.item()
            train_batches += 1
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_batches = 0
        
        with torch.no_grad():
            for batch_features, batch_targets in val_loader:
                batch_features = batch_features.to(device)
                batch_targets = batch_targets.to(device)
                
                outputs = model(
                    financial_data=batch_features,
                    target_prices=batch_targets,
                    return_dict=True
                )
                
                val_loss += outputs['loss'].item()
                val_batches += 1
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_train_loss = train_loss / train_batches
        avg_val_loss = val_loss / val_batches
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), 'best_financial_model.pth')
        
        # æ‰“å°è¿›åº¦
        print(f"Epoch {epoch+1:2d}/{num_epochs} | "
              f"Train Loss: {avg_train_loss:.6f} | "
              f"Val Loss: {avg_val_loss:.6f} | "
              f"LR: {scheduler.get_last_lr()[0]:.2e}")
    
    print("âœ… è®­ç»ƒå®Œæˆ!")
    
    # 7. æµ‹è¯•é¢„æµ‹
    print("\nğŸ”® æµ‹è¯•é¢„æµ‹åŠŸèƒ½...")
    model.eval()
    
    # ä½¿ç”¨éªŒè¯é›†çš„ç¬¬ä¸€ä¸ªæ ·æœ¬è¿›è¡Œé¢„æµ‹
    test_features = val_features[:1].to(device)  # å–ä¸€ä¸ªæ ·æœ¬
    test_targets = val_targets[:1].to(device)
    
    with torch.no_grad():
        predictions = model.predict(test_features, return_dict=True)
        predicted_prices = predictions['predictions']
        
        # åæ ‡å‡†åŒ–é¢„æµ‹ç»“æœ
        predicted_prices = processor.denormalize_predictions(predicted_prices)
        actual_prices = processor.denormalize_predictions(test_targets)
        
        print(f"ğŸ“Š é¢„æµ‹ç»“æœå¯¹æ¯”:")
        print(f"  å®é™…ä»·æ ¼: {actual_prices[0].cpu().numpy()}")
        print(f"  é¢„æµ‹ä»·æ ¼: {predicted_prices[0].cpu().numpy()}")
        
        # è®¡ç®—è¯¯å·®
        mae = torch.mean(torch.abs(predicted_prices - actual_prices)).item()
        mse = torch.mean((predicted_prices - actual_prices) ** 2).item()
        
        print(f"  å¹³å‡ç»å¯¹è¯¯å·® (MAE): {mae:.4f}")
        print(f"  å‡æ–¹è¯¯å·® (MSE): {mse:.4f}")

def main():
    """ä¸»å‡½æ•°"""
    try:
        train_model()
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
