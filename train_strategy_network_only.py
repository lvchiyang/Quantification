"""
ç­–ç•¥ç½‘ç»œç‹¬ç«‹è®­ç»ƒè„šæœ¬
ä¸“é—¨è®­ç»ƒGRUæ¶æ„çš„äº¤æ˜“ç­–ç•¥æ¨¡å‹
"""

import sys
import os
sys.path.append('src')

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt

from strategy_network.config import StrategyNetworkConfigs
from strategy_network.data_processor import StrategyNetworkDataProcessor
from strategy_network.gru_strategy import GRUStrategyNetwork
from strategy_network.strategy_loss import StrategyLoss

def create_strategy_dataloader(features, positions, returns, batch_size, shuffle=True):
    """åˆ›å»ºç­–ç•¥æ•°æ®åŠ è½½å™¨"""
    dataset = TensorDataset(features, positions, returns)
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)

def train_strategy_network_model(config):
    """è®­ç»ƒç­–ç•¥ç½‘ç»œæ¨¡å‹"""
    print("ğŸ§  å¼€å§‹è®­ç»ƒç­–ç•¥ç½‘ç»œæ¨¡å‹")
    print("="*50)
    
    # 1. åˆ›å»ºæ•°æ®å¤„ç†å™¨
    print("ğŸ“Š åŠ è½½æ•°æ®...")
    data_processor = StrategyNetworkDataProcessor(
        data_dir=config.data_dir,
        trading_horizon=config.trading_horizon,
        feature_extraction_length=config.feature_extraction_length,
        large_value_transform=config.large_value_transform
    )
    
    # 2. åŠ è½½æ‰€æœ‰è‚¡ç¥¨æ•°æ®
    stock_data = data_processor.load_all_stocks_for_strategy()
    
    if not stock_data:
        print("âŒ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ•°æ®")
        return None
    
    # 3. åˆå¹¶æ‰€æœ‰è‚¡ç¥¨æ•°æ®
    all_features = []
    all_positions = []
    all_returns = []
    
    for stock_name, (features, positions, returns) in stock_data.items():
        all_features.append(features)
        all_positions.append(positions)
        all_returns.append(returns)
        print(f"  {stock_name}: {features.shape[0]} ä¸ªåºåˆ—")
    
    # åˆå¹¶æ•°æ®
    train_features = torch.cat(all_features, dim=0)
    train_positions = torch.cat(all_positions, dim=0)
    train_returns = torch.cat(all_returns, dim=0)
    
    print(f"\nğŸ“ˆ è®­ç»ƒæ•°æ®ç»Ÿè®¡:")
    print(f"  ç­–ç•¥ç‰¹å¾å½¢çŠ¶: {train_features.shape}")
    print(f"  ä»“ä½ç›®æ ‡å½¢çŠ¶: {train_positions.shape}")
    print(f"  æ”¶ç›Šç‡å½¢çŠ¶: {train_returns.shape}")
    print(f"  æ€»åºåˆ—æ•°: {len(train_features)}")
    
    # 4. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = create_strategy_dataloader(
        train_features, train_positions, train_returns, config.batch_size
    )
    
    # 5. åˆ›å»ºæ¨¡å‹
    print(f"\nğŸ—ï¸  åˆ›å»ºç­–ç•¥ç½‘ç»œ...")
    model = GRUStrategyNetwork(config)
    
    # è®¡ç®—å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"  æ€»å‚æ•°æ•°: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    # 6. è®¾ç½®ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.AdamW(
        model.parameters(),
        lr=config.learning_rate,
        weight_decay=config.weight_decay
    )
    
    # ç­–ç•¥æŸå¤±å‡½æ•°
    criterion = StrategyLoss(config)
    
    # 7. è®­ç»ƒå¾ªç¯
    print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ...")
    train_losses = []
    best_loss = float('inf')
    patience_counter = 0
    
    for epoch in range(config.max_epochs):
        model.train()
        epoch_losses = []
        epoch_returns = []
        
        # è®­ç»ƒä¸€ä¸ªepoch
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{config.max_epochs}")
        for batch_idx, (features, positions, returns) in enumerate(pbar):
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            outputs = model.forward_sequence(features)
            position_predictions = outputs['position_output']['positions']
            
            # è®¡ç®—ç­–ç•¥æŸå¤±
            loss_dict = criterion(position_predictions, returns)
            total_loss = loss_dict['loss_tensor']
            
            # åå‘ä¼ æ’­
            total_loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            # è®°å½•æŸå¤±å’Œæ”¶ç›Š
            epoch_losses.append(total_loss.item())
            if 'cumulative_return' in loss_dict:
                epoch_returns.append(loss_dict['cumulative_return'])
            
            # æ›´æ–°Gumbelæ¸©åº¦
            if hasattr(model, 'update_temperature'):
                model.update_temperature(config.temperature_decay, config.min_temperature)
            
            pbar.set_postfix({
                'loss': f'{total_loss.item():.6f}',
                'return': f'{loss_dict.get("cumulative_return", 0):.4f}'
            })
        
        # è®¡ç®—å¹³å‡æŸå¤±å’Œæ”¶ç›Š
        avg_loss = np.mean(epoch_losses)
        avg_return = np.mean(epoch_returns) if epoch_returns else 0
        train_losses.append(avg_loss)
        
        print(f"Epoch {epoch+1}: æŸå¤± = {avg_loss:.6f}, å¹³å‡æ”¶ç›Š = {avg_return:.4f}")
        
        # æ—©åœæ£€æŸ¥ï¼ˆåŸºäºæŸå¤±ï¼Œä½†ç­–ç•¥ç½‘ç»œå¯èƒ½éœ€è¦æ›´å¤æ‚çš„è¯„ä¼°ï¼‰
        if avg_loss < best_loss:
            best_loss = avg_loss
            patience_counter = 0
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            os.makedirs(config.save_dir, exist_ok=True)
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': best_loss,
                'avg_return': avg_return,
                'config': config
            }, os.path.join(config.save_dir, 'best_strategy_model.pth'))
            
        else:
            patience_counter += 1
            if patience_counter >= config.early_stopping_patience:
                print(f"æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch+1} è½®åœæ­¢è®­ç»ƒ")
                break
        
        # å®šæœŸä¿å­˜
        if (epoch + 1) % config.save_every_n_epochs == 0:
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_loss,
                'avg_return': avg_return,
                'config': config
            }, os.path.join(config.save_dir, f'strategy_model_epoch_{epoch+1}.pth'))
    
    # 8. ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='è®­ç»ƒæŸå¤±')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('ç­–ç•¥ç½‘ç»œè®­ç»ƒæŸå¤±')
    plt.legend()
    plt.grid(True)
    
    if epoch_returns:
        plt.subplot(1, 2, 2)
        plt.plot(epoch_returns, label='å¹³å‡æ”¶ç›Š', color='green')
        plt.xlabel('Epoch')
        plt.ylabel('Return')
        plt.title('ç­–ç•¥ç½‘ç»œæ”¶ç›Šè¡¨ç°')
        plt.legend()
        plt.grid(True)
    
    plt.tight_layout()
    plt.savefig(os.path.join(config.save_dir, 'training_curve.png'))
    plt.show()
    
    print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³æŸå¤±: {best_loss:.6f}")
    print(f"æ¨¡å‹ä¿å­˜åœ¨: {config.save_dir}")
    
    return model, train_losses

def main():
    """ä¸»å‡½æ•°"""
    # é€‰æ‹©é…ç½®
    print("é€‰æ‹©ç­–ç•¥ç½‘ç»œé…ç½®:")
    print("1. tiny - è½»é‡çº§æµ‹è¯•")
    print("2. small - å°å‹æ¨¡å‹")
    print("3. base - åŸºç¡€æ¨¡å‹ï¼ˆæ¨èï¼‰")
    print("4. large - å¤§å‹æ¨¡å‹")
    print("5. conservative - ä¿å®ˆç­–ç•¥")
    print("6. aggressive - æ¿€è¿›ç­–ç•¥")
    print("7. high_frequency - é«˜é¢‘äº¤æ˜“")
    print("8. long_term - é•¿æœŸæŠ•èµ„")
    
    choice = input("è¯·é€‰æ‹©é…ç½® (1-8, é»˜è®¤3): ").strip() or "3"
    
    config_map = {
        "1": StrategyNetworkConfigs.tiny(),
        "2": StrategyNetworkConfigs.small(),
        "3": StrategyNetworkConfigs.base(),
        "4": StrategyNetworkConfigs.large(),
        "5": StrategyNetworkConfigs.conservative(),
        "6": StrategyNetworkConfigs.aggressive(),
        "7": StrategyNetworkConfigs.high_frequency(),
        "8": StrategyNetworkConfigs.long_term()
    }
    
    config = config_map.get(choice, StrategyNetworkConfigs.base())
    
    # å¼€å§‹è®­ç»ƒ
    try:
        model, losses = train_strategy_network_model(config)
        
        if model is not None:
            print("\nğŸ‰ ç­–ç•¥ç½‘ç»œè®­ç»ƒæˆåŠŸ!")
            print("ç°åœ¨å¯ä»¥ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œäº¤æ˜“ç­–ç•¥å†³ç­–äº†ã€‚")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
