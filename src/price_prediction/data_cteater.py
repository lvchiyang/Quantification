# -*- coding: utf-8 -*-
"""
åºåˆ—å¤„ç†å™¨ - æœ€ç»ˆç‰ˆ
ç”¨äºåˆ›å»ºè®­ç»ƒå’Œé¢„æµ‹åºåˆ—ï¼Œé¿å…æ•°æ®æ³„éœ²
"""

import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset
import glob
import os
from typing import Tuple, List, Optional


class SequenceProcessor:
    """
    åºåˆ—çº§æ•°æ®å¤„ç†å™¨
    æ ¸å¿ƒï¼šæ¯ä¸ª180å¤©åºåˆ—ç‹¬ç«‹è®¡ç®—åŸºå‡†ï¼Œé¿å…æ•°æ®æ³„éœ²
    """
    
    def __init__(self, sequence_length: int = 180):
        self.sequence_length = sequence_length
        self.feature_columns = [
            'æœˆ', 'æ—¥', 'æ˜ŸæœŸ',                           # æ—¶é—´ç‰¹å¾ (3ç»´)
            'open_rel', 'high_rel', 'low_rel', 'close_rel',  # ä»·æ ¼ç‰¹å¾ (4ç»´)
            'æ¶¨å¹…', 'æŒ¯å¹…',                               # ä»·æ ¼å˜åŒ– (2ç»´)
            'volume_rel', 'volume_change',                # æˆäº¤é‡ç‰¹å¾ (2ç»´)
            'amount_rel', 'amount_change',                # é‡‘é¢ç‰¹å¾ (2ç»´)
            'æ¢æ‰‹%', 'æˆäº¤æ¬¡æ•°',                          # å¸‚åœºæ´»è·ƒåº¦ (2ç»´)
            'big_order_activity', 'chip_concentration',   # é‡‘èç‰¹å¾1,2 (2ç»´)
            'market_sentiment', 'price_volume_sync'       # é‡‘èç‰¹å¾3,4 (2ç»´)
        ]  # æ€»è®¡20ç»´ç‰¹å¾
        
    def process_sequence_features(self, sequence_df: pd.DataFrame) -> dict:
        """
        å¯¹å•ä¸ª180å¤©åºåˆ—è¿›è¡Œç‰¹å¾å·¥ç¨‹
        å…³é”®ï¼šä½¿ç”¨åºåˆ—å†…æ•°æ®è®¡ç®—åŸºå‡†ï¼Œé¿å…æ•°æ®æ³„éœ²
        """
        # 1. åºåˆ—çº§ä»·æ ¼ç‰¹å¾å¤„ç†
        ohlc_data = sequence_df[['å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜']].copy()
        for col in ['å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜']:
            ohlc_data[col] = pd.to_numeric(ohlc_data[col], errors='coerce')
        
        # è®¡ç®—åºåˆ—å†…æ‰€æœ‰OHLCçš„ä¸­ä½æ•°ä½œä¸ºä»·æ ¼åŸºå‡†
        all_prices = ohlc_data.values.flatten()
        all_prices = all_prices[~pd.isna(all_prices)]
        sequence_price_median = np.median(all_prices)
        
        # OHLCç›¸å¯¹äºåºåˆ—åŸºå‡†çš„æ¯”å€¼
        open_rel = (sequence_df['å¼€ç›˜'] / sequence_price_median).fillna(1.0)
        high_rel = (sequence_df['æœ€é«˜'] / sequence_price_median).fillna(1.0)
        low_rel = (sequence_df['æœ€ä½'] / sequence_price_median).fillna(1.0)
        close_rel = (sequence_df['æ”¶ç›˜'] / sequence_price_median).fillna(1.0)
        
        # 2. åºåˆ—çº§æˆäº¤é‡/é‡‘é¢å¤„ç†
        def process_volume_amount(col_name):
            values = pd.to_numeric(sequence_df[col_name], errors='coerce').fillna(0)
            
            # åºåˆ—å†…ä¸­ä½æ•°åŸºå‡†
            sequence_median = values.median()
            if sequence_median == 0:
                sequence_median = 1.0
            relative_values = values / sequence_median
            
            # ç›¸å¯¹å˜åŒ–ç‡ï¼ˆ20æ—¥æ»šåŠ¨å‡å€¼ï¼‰
            rolling_mean = values.rolling(window=20, min_periods=1).mean()
            rolling_mean = rolling_mean.fillna(method='bfill').fillna(1.0).replace(0, 1.0)
            relative_change = ((values - rolling_mean) / rolling_mean * 100).fillna(0.0)
            
            return relative_values, relative_change
        
        volume_rel, volume_change = process_volume_amount('æ€»æ‰‹')
        amount_rel, amount_change = process_volume_amount('é‡‘é¢')
        
        # 3. åºåˆ—çº§é‡‘èç‰¹å¾å¤„ç†
        def standardize_without_clipping(series):
            mean = series.mean()
            std = series.std() + 1e-6
            return (series - mean) / std
        
        # ç¡®ä¿æ•°å€¼ç±»å‹
        total_volume = pd.to_numeric(sequence_df['æ€»æ‰‹'], errors='coerce').fillna(0)
        trade_count = pd.to_numeric(sequence_df['æˆäº¤æ¬¡æ•°'], errors='coerce').fillna(1)
        turnover_rate = pd.to_numeric(sequence_df['æ¢æ‰‹%'], errors='coerce').fillna(0)
        price_change = pd.to_numeric(sequence_df['æ¶¨å¹…'], errors='coerce').fillna(0)
        amplitude = pd.to_numeric(sequence_df['æŒ¯å¹…'], errors='coerce').fillna(0)
        
        # é‡‘èç‰¹å¾è®¡ç®—
        big_order_activity = total_volume / (trade_count + 1e-6)
        
        volume_mean = total_volume.rolling(30, min_periods=1).mean().fillna(method='bfill').fillna(1.0)
        volume_normalized = total_volume / (volume_mean + 1e-6)
        chip_concentration = turnover_rate / (volume_normalized + 1e-6)
        
        market_sentiment = price_change * amplitude / 100
        
        price_direction = np.sign(price_change)
        volume_change_pct = total_volume.pct_change().fillna(0)
        volume_direction = np.sign(volume_change_pct)
        price_volume_sync = price_direction * volume_direction
        
        # æ ‡å‡†åŒ–ä½†ä¸è£å‰ª
        big_order_activity = standardize_without_clipping(big_order_activity)
        chip_concentration = standardize_without_clipping(chip_concentration)
        market_sentiment = standardize_without_clipping(market_sentiment)
        
        return {
            'open_rel': open_rel,
            'high_rel': high_rel,
            'low_rel': low_rel,
            'close_rel': close_rel,
            'volume_rel': volume_rel,
            'volume_change': volume_change,
            'amount_rel': amount_rel,
            'amount_change': amount_change,
            'big_order_activity': big_order_activity,
            'chip_concentration': chip_concentration,
            'market_sentiment': market_sentiment,
            'price_volume_sync': price_volume_sync
        }
    
    def build_feature_vector(self, sequence_df: pd.DataFrame, computed_features: dict) -> np.ndarray:
        """
        æ„å»º20ç»´ç‰¹å¾å‘é‡
        """
        # åŸºç¡€ç‰¹å¾ï¼ˆæ¥è‡ªåŸå§‹æ•°æ®ï¼‰
        result_df = sequence_df[['æœˆ', 'æ—¥', 'æ˜ŸæœŸ', 'æ¶¨å¹…', 'æŒ¯å¹…', 'æ¢æ‰‹%', 'æˆäº¤æ¬¡æ•°']].copy()
        
        # æ·»åŠ è®¡ç®—ç‰¹å¾
        for feature_name, feature_values in computed_features.items():
            result_df[feature_name] = feature_values
        
        # æŒ‰ç…§æŒ‡å®šé¡ºåºé€‰æ‹©ç‰¹å¾
        return result_df[self.feature_columns].values  # [180, 20]
    
    def create_training_sequences(self, cleaned_data: pd.DataFrame) -> List[Tuple[np.ndarray, np.ndarray]]:
        """
        ä»æ¸…æ´—åçš„æ•°æ®åˆ›å»ºè®­ç»ƒåºåˆ—
        
        Args:
            cleaned_data: åŸºç¡€æ¸…æ´—åçš„æ•°æ®ï¼ˆ14åˆ—ï¼‰
            
        Returns:
            List of (input_sequence, target_prices)
            input_sequence: [180, 20] ç‰¹å¾çŸ©é˜µ
            target_prices: [10] æœªæ¥10ä¸ªæ—¶é—´ç‚¹çš„close_rel
        """
        sequences = []
        target_days = [1, 2, 3, 4, 5, 10, 15, 20, 25, 30]  # æœªæ¥ç¬¬Nå¤©
        max_target_day = max(target_days)
        
        for i in range(len(cleaned_data) - self.sequence_length - max_target_day + 1):
            # æå–180å¤©åºåˆ—
            sequence = cleaned_data.iloc[i:i+self.sequence_length].copy()
            
            # åºåˆ—çº§ç‰¹å¾å·¥ç¨‹ï¼ˆé¿å…æ•°æ®æ³„éœ²ï¼‰
            computed_features = self.process_sequence_features(sequence)
            
            # æ„å»ºç‰¹å¾å‘é‡
            feature_vector = self.build_feature_vector(sequence, computed_features)
            
            # æå–ç›®æ ‡ä»·æ ¼ï¼ˆæœªæ¥æŒ‡å®šå¤©æ•°çš„æ”¶ç›˜ä»·ç›¸å¯¹å€¼ï¼‰
            target_prices = []
            for day in target_days:
                target_idx = i + self.sequence_length + day - 1
                if target_idx < len(cleaned_data):
                    # è®¡ç®—ç›®æ ‡åºåˆ—çš„ä»·æ ¼åŸºå‡†ï¼ˆç”¨äºè®¡ç®—targetçš„close_relï¼‰
                    target_sequence = cleaned_data.iloc[i+day:i+day+self.sequence_length]
                    if len(target_sequence) >= self.sequence_length:
                        target_features = self.process_sequence_features(target_sequence)
                        target_close_rel = target_features['close_rel'].iloc[-1]  # æœ€åä¸€å¤©çš„close_rel
                        target_prices.append(target_close_rel)
                    else:
                        target_prices.append(1.0)  # é»˜è®¤å€¼
                else:
                    target_prices.append(1.0)  # é»˜è®¤å€¼
            
            sequences.append((feature_vector, np.array(target_prices)))
        
        return sequences
    
    def create_prediction_sequence(self, recent_data: pd.DataFrame) -> np.ndarray:
        """
        åˆ›å»ºé¢„æµ‹åºåˆ—ï¼ˆæœ€å180å¤©ï¼‰
        
        Args:
            recent_data: æœ€è¿‘çš„æ•°æ®ï¼ˆè‡³å°‘180è¡Œï¼‰
            
        Returns:
            feature_vector: [180, 20] ç‰¹å¾çŸ©é˜µ
        """
        if len(recent_data) < self.sequence_length:
            raise ValueError(f"æ•°æ®é•¿åº¦ä¸è¶³ï¼Œéœ€è¦è‡³å°‘{self.sequence_length}å¤©æ•°æ®")
        
        # å–æœ€å180å¤©
        sequence = recent_data.iloc[-self.sequence_length:].copy()
        
        # åºåˆ—çº§ç‰¹å¾å·¥ç¨‹
        computed_features = self.process_sequence_features(sequence)
        
        # æ„å»ºç‰¹å¾å‘é‡
        feature_vector = self.build_feature_vector(sequence, computed_features)
        
        return feature_vector  # [180, 20]


class PriceDataset(Dataset):
    """
    ä»·æ ¼é¢„æµ‹æ•°æ®é›†
    """
    
    def __init__(self, data_dir: str, sequence_length: int = 180):
        self.processor = SequenceProcessor(sequence_length)
        self.data = []
        
        print(f"åŠ è½½æ•°æ®ç›®å½•: {data_dir}")
        
        # åŠ è½½æ‰€æœ‰æ¸…æ´—åçš„æ•°æ®æ–‡ä»¶
        for data_file in glob.glob(f"{data_dir}/**/*.xlsx", recursive=True):
            try:
                df = pd.read_excel(data_file)
                print(f"å¤„ç†æ–‡ä»¶: {os.path.basename(data_file)}, æ•°æ®é•¿åº¦: {len(df)}")
                
                # åˆ›å»ºè®­ç»ƒåºåˆ—
                sequences = self.processor.create_training_sequences(df)
                self.data.extend(sequences)
                
                print(f"  ç”Ÿæˆåºåˆ—æ•°: {len(sequences)}")
                
            except Exception as e:
                print(f"å¤„ç†æ–‡ä»¶å¤±è´¥: {data_file}, é”™è¯¯: {e}")
        
        print(f"æ€»åºåˆ—æ•°: {len(self.data)}")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        input_seq, target_prices = self.data[idx]
        return torch.FloatTensor(input_seq), torch.FloatTensor(target_prices)


def predict_stock_price(model, stock_file: str, processor: SequenceProcessor = None):
    """
    å¯¹å•åªè‚¡ç¥¨è¿›è¡Œä»·æ ¼é¢„æµ‹
    
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        stock_file: è‚¡ç¥¨æ•°æ®æ–‡ä»¶è·¯å¾„
        processor: åºåˆ—å¤„ç†å™¨
        
    Returns:
        predictions: [10] æœªæ¥10ä¸ªæ—¶é—´ç‚¹çš„é¢„æµ‹å€¼
    """
    if processor is None:
        processor = SequenceProcessor()
    
    # åŠ è½½æ•°æ®
    df = pd.read_excel(stock_file)
    
    # åˆ›å»ºé¢„æµ‹åºåˆ—
    input_seq = processor.create_prediction_sequence(df)
    input_tensor = torch.FloatTensor(input_seq).unsqueeze(0)  # [1, 180, 20]
    
    # é¢„æµ‹
    model.eval()
    with torch.no_grad():
        outputs = model(input_tensor)
        if isinstance(outputs, dict):
            predictions = outputs['price_predictions']  # [1, 10]
        else:
            predictions = outputs  # [1, 10]
    
    return predictions.squeeze().cpu().numpy()  # [10]


def validate_sequence_processing(cleaned_data: pd.DataFrame, processor: SequenceProcessor = None):
    """
    éªŒè¯åºåˆ—å¤„ç†æ˜¯å¦é¿å…äº†æ•°æ®æ³„éœ²
    """
    if processor is None:
        processor = SequenceProcessor()

    print("ğŸ” éªŒè¯åºåˆ—å¤„ç†...")

    # æ£€æŸ¥ä¸åŒåºåˆ—çš„ä»·æ ¼åŸºå‡†
    if len(cleaned_data) >= 280:
        sequence1 = cleaned_data.iloc[0:180]
        sequence2 = cleaned_data.iloc[100:280]

        features1 = processor.process_sequence_features(sequence1)
        features2 = processor.process_sequence_features(sequence2)

        # è®¡ç®—ä»·æ ¼åŸºå‡†
        median1 = np.median(sequence1[['å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜']].values.flatten())
        median2 = np.median(sequence2[['å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜']].values.flatten())

        print(f"åºåˆ—1ä»·æ ¼åŸºå‡†: {median1:.2f}")
        print(f"åºåˆ—2ä»·æ ¼åŸºå‡†: {median2:.2f}")
        print(f"åŸºå‡†å·®å¼‚: {abs(median1 - median2):.2f}")

        # æ£€æŸ¥ç‰¹å¾èŒƒå›´ï¼ˆä¸åº”è¯¥è¢«è£å‰ªï¼‰
        print(f"åºåˆ—1å¤§å•æ´»è·ƒåº¦èŒƒå›´: [{features1['big_order_activity'].min():.3f}, {features1['big_order_activity'].max():.3f}]")
        print(f"åºåˆ—2å¤§å•æ´»è·ƒåº¦èŒƒå›´: [{features2['big_order_activity'].min():.3f}, {features2['big_order_activity'].max():.3f}]")

        # æ£€æŸ¥æ˜¯å¦æœ‰NaNå€¼
        feature_vector1 = processor.build_feature_vector(sequence1, features1)
        feature_vector2 = processor.build_feature_vector(sequence2, features2)

        has_nan1 = np.isnan(feature_vector1).any()
        has_nan2 = np.isnan(feature_vector2).any()

        print(f"åºåˆ—1åŒ…å«NaN: {has_nan1}")
        print(f"åºåˆ—2åŒ…å«NaN: {has_nan2}")

        if not has_nan1 and not has_nan2:
            print("âœ… éªŒè¯é€šè¿‡ï¼šæ— æ•°æ®æ³„éœ²ï¼Œæ— NaNå€¼")
        else:
            print("âŒ éªŒè¯å¤±è´¥ï¼šå­˜åœ¨NaNå€¼")
    else:
        print("âŒ æ•°æ®é•¿åº¦ä¸è¶³ï¼Œæ— æ³•éªŒè¯")


def check_data_quality(sequences: List[Tuple[np.ndarray, np.ndarray]]):
    """
    æ£€æŸ¥å¤„ç†åæ•°æ®çš„è´¨é‡
    """
    print("ğŸ“Š æ•°æ®è´¨é‡æ£€æŸ¥...")
    print(f"åºåˆ—æ•°é‡: {len(sequences)}")

    if len(sequences) > 0:
        input_seq, target_seq = sequences[0]
        print(f"è¾“å…¥åºåˆ—å½¢çŠ¶: {input_seq.shape}")  # åº”è¯¥æ˜¯ (180, 20)
        print(f"ç›®æ ‡åºåˆ—å½¢çŠ¶: {target_seq.shape}")  # åº”è¯¥æ˜¯ (10,)

        # æ£€æŸ¥æ˜¯å¦æœ‰NaNå€¼
        all_inputs = np.array([seq[0] for seq in sequences])
        all_targets = np.array([seq[1] for seq in sequences])

        has_nan_inputs = np.isnan(all_inputs).any()
        has_nan_targets = np.isnan(all_targets).any()

        print(f"è¾“å…¥åŒ…å«NaNå€¼: {has_nan_inputs}")
        print(f"ç›®æ ‡åŒ…å«NaNå€¼: {has_nan_targets}")

        # æ£€æŸ¥æ•°å€¼èŒƒå›´
        feature_names = [
            'æœˆ', 'æ—¥', 'æ˜ŸæœŸ', 'open_rel', 'high_rel', 'low_rel', 'close_rel',
            'æ¶¨å¹…', 'æŒ¯å¹…', 'volume_rel', 'volume_change', 'amount_rel', 'amount_change',
            'æ¢æ‰‹%', 'æˆäº¤æ¬¡æ•°', 'big_order_activity', 'chip_concentration',
            'market_sentiment', 'price_volume_sync'
        ]

        print("\nç‰¹å¾æ•°å€¼èŒƒå›´:")
        for i, feature_name in enumerate(feature_names):
            feature_values = all_inputs[:, :, i].flatten()
            print(f"  {feature_name}: [{feature_values.min():.3f}, {feature_values.max():.3f}]")

        print(f"\nç›®æ ‡å€¼èŒƒå›´: [{all_targets.min():.3f}, {all_targets.max():.3f}]")


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
if __name__ == "__main__":
    print("ğŸš€ åºåˆ—å¤„ç†å™¨æµ‹è¯•")

    # æµ‹è¯•å•ä¸ªæ–‡ä»¶
    test_file = "processed_data_2025-07-30/è‚¡ç¥¨æ•°æ®/ç™½é…’/èŒ…å°.xlsx"
    if os.path.exists(test_file):
        print(f"\nğŸ“ æµ‹è¯•æ–‡ä»¶: {test_file}")

        # åŠ è½½æ•°æ®
        df = pd.read_excel(test_file)
        print(f"æ•°æ®å½¢çŠ¶: {df.shape}")

        # åˆ›å»ºå¤„ç†å™¨
        processor = SequenceProcessor()

        # éªŒè¯åºåˆ—å¤„ç†
        validate_sequence_processing(df, processor)

        # åˆ›å»ºè®­ç»ƒåºåˆ—
        print(f"\nğŸ”„ åˆ›å»ºè®­ç»ƒåºåˆ—...")
        sequences = processor.create_training_sequences(df)
        print(f"ç”Ÿæˆåºåˆ—æ•°: {len(sequences)}")

        # æ£€æŸ¥æ•°æ®è´¨é‡
        if len(sequences) > 0:
            check_data_quality(sequences[:100])  # æ£€æŸ¥å‰100ä¸ªåºåˆ—

        # æµ‹è¯•é¢„æµ‹åºåˆ—åˆ›å»º
        print(f"\nğŸ”® æµ‹è¯•é¢„æµ‹åºåˆ—åˆ›å»º...")
        try:
            pred_seq = processor.create_prediction_sequence(df)
            print(f"é¢„æµ‹åºåˆ—å½¢çŠ¶: {pred_seq.shape}")
            print("âœ… é¢„æµ‹åºåˆ—åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            print(f"âŒ é¢„æµ‹åºåˆ—åˆ›å»ºå¤±è´¥: {e}")

    else:
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")

        # åˆ›å»ºå®Œæ•´æ•°æ®é›†æµ‹è¯•
        data_dir = "processed_data_2025-07-30/è‚¡ç¥¨æ•°æ®"
        if os.path.exists(data_dir):
            print(f"\nğŸ“‚ æµ‹è¯•æ•°æ®ç›®å½•: {data_dir}")
            dataset = PriceDataset(data_dir)

            if len(dataset) > 0:
                # åˆ›å»ºæ•°æ®åŠ è½½å™¨
                from torch.utils.data import DataLoader
                dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

                # æ£€æŸ¥æ•°æ®
                for batch_idx, (inputs, targets) in enumerate(dataloader):
                    print(f"Batch {batch_idx}: inputs {inputs.shape}, targets {targets.shape}")
                    print(f"  è¾“å…¥èŒƒå›´: [{inputs.min():.3f}, {inputs.max():.3f}]")
                    print(f"  ç›®æ ‡èŒƒå›´: [{targets.min():.3f}, {targets.max():.3f}]")
                    if batch_idx == 0:
                        break
            else:
                print("âŒ æ•°æ®é›†ä¸ºç©º")
        else:
            print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
