# -*- coding: utf-8 -*-
"""
åºåˆ—å¤„ç†å™¨ - æœ€ç»ˆç‰ˆ
ç”¨äºåˆ›å»ºè®­ç»ƒå’Œé¢„æµ‹åºåˆ—ï¼Œé¿å…æ•°æ®æ³„éœ²
"""

import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
import glob
import os
from typing import Tuple, List, Optional


class SequenceProcessor:
    """
    åºåˆ—çº§æ•°æ®å¤„ç†å™¨
    æ ¸å¿ƒï¼šæ¯ä¸ª180å¤©åºåˆ—ç‹¬ç«‹è®¡ç®—åŸºå‡†ï¼Œé¿å…æ•°æ®æ³„éœ²
    """

    def __init__(self, sequence_length: int = 180, predict_relative: bool = True):
        self.sequence_length = sequence_length
        self.predict_relative = predict_relative  # True: é¢„æµ‹æ¯”å€¼, False: é¢„æµ‹ç»å¯¹ä»·æ ¼
        self.feature_columns = [
            'æœˆ', 'æ—¥', 'æ˜ŸæœŸ',                           # æ—¶é—´ç‰¹å¾ (3ç»´)
            'open_rel', 'high_rel', 'low_rel', 'close_rel',  # ä»·æ ¼ç‰¹å¾ (4ç»´)
            'æ¶¨å¹…', 'æŒ¯å¹…',                               # ä»·æ ¼å˜åŒ– (2ç»´)
            'volume_rel', 'volume_log',                   # æˆäº¤é‡ç‰¹å¾ (2ç»´)
            'amount_rel', 'amount_log',                   # é‡‘é¢ç‰¹å¾ (2ç»´)
            'æˆäº¤æ¬¡æ•°',                                   # æˆäº¤é‡ç›¸å…³ç‰¹å¾ (1ç»´)
            'æ¢æ‰‹%',                                     # å¸‚åœºç‰¹å¾ (1ç»´)
            'price_median',                              # ä»·æ ¼åŸºå‡† (1ç»´)
            'big_order_activity', 'chip_concentration',   # é‡‘èç‰¹å¾1,2 (2ç»´)
            'market_sentiment', 'price_volume_sync'       # é‡‘èç‰¹å¾3,4 (2ç»´)
        ]  # æ€»è®¡20ç»´ç‰¹å¾
        
    def process_sequence_features(self, sequence_df: pd.DataFrame) -> dict:
        """
        å¯¹å•ä¸ª180å¤©åºåˆ—è¿›è¡Œç‰¹å¾å·¥ç¨‹
        å…³é”®ï¼šä½¿ç”¨åºåˆ—å†…æ•°æ®è®¡ç®—åŸºå‡†ï¼Œé¿å…æ•°æ®æ³„éœ²
        """
        # 1. åºåˆ—çº§ä»·æ ¼ç‰¹å¾å¤„ç†
        ohlc_data = sequence_df[['å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜']].copy()
        for col in ['å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜']:
            ohlc_data[col] = pd.to_numeric(ohlc_data[col], errors='coerce')
        
        # è®¡ç®—åºåˆ—å†…æ‰€æœ‰OHLCçš„ä¸­ä½æ•°ä½œä¸ºä»·æ ¼åŸºå‡†
        all_prices = ohlc_data.values.flatten()
        all_prices = all_prices[~pd.isna(all_prices)]
        sequence_price_median = np.median(all_prices)
        
        # OHLCç›¸å¯¹äºåºåˆ—åŸºå‡†çš„æ¯”å€¼
        open_rel = (sequence_df['å¼€ç›˜'] / sequence_price_median).fillna(1.0)
        high_rel = (sequence_df['æœ€é«˜'] / sequence_price_median).fillna(1.0)
        low_rel = (sequence_df['æœ€ä½'] / sequence_price_median).fillna(1.0)
        close_rel = (sequence_df['æ”¶ç›˜'] / sequence_price_median).fillna(1.0)
        
        # 2. åºåˆ—çº§æˆäº¤é‡/é‡‘é¢å¤„ç†
        def process_volume_amount(col_name):
            values = pd.to_numeric(sequence_df[col_name], errors='coerce').fillna(0)

            # åºåˆ—å†…ä¸­ä½æ•°åŸºå‡† - ç›¸å¯¹å€¼
            sequence_median = values.median()
            if sequence_median == 0:
                sequence_median = 1.0
            relative_values = values / sequence_median

            # å¯¹æ•°å€¼ - ç»å¯¹æ°´å¹³ä¿¡æ¯
            # ä½¿ç”¨ log1p é¿å… log(0) é—®é¢˜
            log_values = np.log1p(values)  # log(1 + x)

            return relative_values, log_values

        volume_rel, volume_log = process_volume_amount('æ€»æ‰‹')
        amount_rel, amount_log = process_volume_amount('é‡‘é¢')
        
        # 3. åºåˆ—çº§é‡‘èç‰¹å¾å¤„ç†
        def standardize_without_clipping(series):
            mean = series.mean()
            std = series.std() + 1e-6
            return (series - mean) / std
        
        # ç¡®ä¿æ•°å€¼ç±»å‹
        total_volume = pd.to_numeric(sequence_df['æ€»æ‰‹'], errors='coerce').fillna(0)
        trade_count = pd.to_numeric(sequence_df['æˆäº¤æ¬¡æ•°'], errors='coerce').fillna(1)
        turnover_rate = pd.to_numeric(sequence_df['æ¢æ‰‹%'], errors='coerce').fillna(0)
        price_change = pd.to_numeric(sequence_df['æ¶¨å¹…'], errors='coerce').fillna(0)
        amplitude = pd.to_numeric(sequence_df['æŒ¯å¹…'], errors='coerce').fillna(0)
        
        # é‡‘èç‰¹å¾è®¡ç®—
        # å¤§å•æ´»è·ƒåº¦ï¼šå¹³å‡æ¯ç¬”äº¤æ˜“é‡ï¼Œéœ€è¦å¯¹æ•°å¤„ç†å’Œæ ‡å‡†åŒ–
        avg_trade_volume = total_volume / (trade_count + 1e-6)
        big_order_activity = np.log1p(avg_trade_volume)  # å¯¹æ•°å¤„ç†å‹ç¼©æ•°å€¼èŒƒå›´
        big_order_activity = standardize_without_clipping(big_order_activity)

        volume_mean = total_volume.rolling(30, min_periods=1).mean().fillna(method='bfill').fillna(1.0)
        volume_normalized = total_volume / (volume_mean + 1e-6)
        chip_concentration = turnover_rate / (volume_normalized + 1e-6)
        chip_concentration = standardize_without_clipping(chip_concentration)

        market_sentiment = price_change * amplitude / 100
        market_sentiment = standardize_without_clipping(market_sentiment)

        price_direction = np.sign(price_change)
        volume_change_pct = total_volume.pct_change().fillna(0)
        volume_direction = np.sign(volume_change_pct)
        price_volume_sync = price_direction * volume_direction  # å·²ç»æ˜¯-1,0,1ï¼Œä¸éœ€è¦æ ‡å‡†åŒ–
        
        # è®¡ç®—ä»·æ ¼åŸºå‡†ï¼ˆåºåˆ—ä¸­ä½æ•°ï¼‰
        sequence_price_median = self._get_sequence_price_median(sequence_df)

        return {
            'open_rel': open_rel,
            'high_rel': high_rel,
            'low_rel': low_rel,
            'close_rel': close_rel,
            'volume_rel': volume_rel,
            'volume_log': volume_log,
            'amount_rel': amount_rel,
            'amount_log': amount_log,
            'price_median': sequence_price_median,  # æ·»åŠ ä»·æ ¼åŸºå‡†
            'big_order_activity': big_order_activity,
            'chip_concentration': chip_concentration,
            'market_sentiment': market_sentiment,
            'price_volume_sync': price_volume_sync
        }
    
    def build_feature_vector(self, sequence_df: pd.DataFrame, computed_features: dict) -> np.ndarray:
        """
        æ„å»º22ç»´ç‰¹å¾å‘é‡ï¼ˆä»·æ ¼åŸºå‡†å•ç‹¬æˆåˆ—ï¼‰
        """
        # åŸºç¡€ç‰¹å¾ï¼ˆæ¥è‡ªåŸå§‹æ•°æ®ï¼‰
        result_df = sequence_df[['æœˆ', 'æ—¥', 'æ˜ŸæœŸ', 'æ¶¨å¹…', 'æŒ¯å¹…', 'æ¢æ‰‹%', 'æˆäº¤æ¬¡æ•°']].copy()

        # æ·»åŠ è®¡ç®—ç‰¹å¾ï¼ˆåŒ…æ‹¬ä»·æ ¼åŸºå‡†ï¼‰
        for feature_name, feature_values in computed_features.items():
            if feature_name == 'price_median':
                # ä»·æ ¼åŸºå‡†æ˜¯æ ‡é‡ï¼Œéœ€è¦æ‰©å±•ä¸ºæ•°ç»„
                result_df[feature_name] = feature_values
            else:
                result_df[feature_name] = feature_values

        # æŒ‰ç…§æŒ‡å®šé¡ºåºé€‰æ‹©ç‰¹å¾
        return result_df[self.feature_columns].values  # [180, 22]
    
    def create_training_sequences(self, cleaned_data: pd.DataFrame) -> List[Tuple[np.ndarray, np.ndarray]]:
        """
        ä»æ¸…æ´—åçš„æ•°æ®åˆ›å»ºè®­ç»ƒåºåˆ—

        Args:
            cleaned_data: åŸºç¡€æ¸…æ´—åçš„æ•°æ®ï¼ˆ14åˆ—ï¼‰

        Returns:
            List of (input_sequence, target_prices)
            input_sequence: [180, 20] ç‰¹å¾çŸ©é˜µï¼ˆå·²åŒ…å«ä»·æ ¼åŸºå‡†ä¿¡æ¯ï¼‰
            target_prices: [10] ç»å¯¹ä»·æ ¼æˆ–ç›¸å¯¹å€¼ï¼ˆæ ¹æ®predict_relativeé…ç½®ï¼‰
        """
        sequences = []
        target_days = [1, 2, 3, 4, 5, 10, 15, 20, 25, 30]  # æœªæ¥ç¬¬Nå¤©
        max_target_day = max(target_days)
        
        for i in range(len(cleaned_data) - self.sequence_length - max_target_day + 1):
            # æå–180å¤©åºåˆ—
            sequence = cleaned_data.iloc[i:i+self.sequence_length].copy()
            
            # åºåˆ—çº§ç‰¹å¾å·¥ç¨‹ï¼ˆé¿å…æ•°æ®æ³„éœ²ï¼‰
            computed_features = self.process_sequence_features(sequence)
            
            # æ„å»ºç‰¹å¾å‘é‡
            feature_vector = self.build_feature_vector(sequence, computed_features)
            
            # æå–ç›®æ ‡ä»·æ ¼ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            target_prices = []
            sequence_price_median = self._get_sequence_price_median(sequence)

            for day in target_days:
                target_idx = i + self.sequence_length + day - 1
                if target_idx < len(cleaned_data):
                    target_close_price = pd.to_numeric(cleaned_data.iloc[target_idx]['æ”¶ç›˜'], errors='coerce')
                    if pd.isna(target_close_price):
                        target_close_price = sequence_price_median

                    if self.predict_relative:
                        # é¢„æµ‹æ¯”å€¼
                        target_prices.append(target_close_price / sequence_price_median)
                    else:
                        # é¢„æµ‹ç»å¯¹ä»·æ ¼
                        target_prices.append(target_close_price)
                else:
                    # æ•°æ®ä¸è¶³æ—¶çš„é»˜è®¤å€¼
                    target_prices.append(1.0 if self.predict_relative else sequence_price_median)

            sequences.append((feature_vector, np.array(target_prices)))
        
        return sequences

    def _get_sequence_price_median(self, sequence_df: pd.DataFrame) -> float:
        """
        è·å–åºåˆ—çš„ä»·æ ¼åŸºå‡†ï¼ˆä¸­ä½æ•°ï¼‰

        Args:
            sequence_df: åºåˆ—æ•°æ®

        Returns:
            ä»·æ ¼åŸºå‡†å€¼
        """
        close_prices = pd.to_numeric(sequence_df['æ”¶ç›˜'], errors='coerce').fillna(method='ffill')
        price_median = close_prices.median()
        if pd.isna(price_median) or price_median <= 0:
            price_median = close_prices.mean()
        if pd.isna(price_median) or price_median <= 0:
            price_median = 100.0  # é»˜è®¤åŸºå‡†ä»·æ ¼
        return float(price_median)

    def create_prediction_sequence(self, recent_data: pd.DataFrame) -> np.ndarray:
        """
        åˆ›å»ºé¢„æµ‹åºåˆ—ï¼ˆæœ€å180å¤©ï¼‰

        Args:
            recent_data: æœ€è¿‘çš„æ•°æ®ï¼ˆè‡³å°‘180è¡Œï¼‰

        Returns:
            feature_vector: [180, 20] ç‰¹å¾çŸ©é˜µï¼ˆå·²åŒ…å«ä»·æ ¼åŸºå‡†ä¿¡æ¯ï¼‰
        """
        if len(recent_data) < self.sequence_length:
            raise ValueError(f"æ•°æ®é•¿åº¦ä¸è¶³ï¼Œéœ€è¦è‡³å°‘{self.sequence_length}å¤©æ•°æ®")

        # å–æœ€å180å¤©
        sequence = recent_data.iloc[-self.sequence_length:].copy()

        # åºåˆ—çº§ç‰¹å¾å·¥ç¨‹
        computed_features = self.process_sequence_features(sequence)

        # æ„å»ºç‰¹å¾å‘é‡ï¼ˆå·²åŒ…å«ä»·æ ¼åŸºå‡†ä¿¡æ¯ï¼‰
        feature_vector = self.build_feature_vector(sequence, computed_features)

        return feature_vector  # [180, 20]


class PriceDataset(Dataset):
    """
    ä»·æ ¼é¢„æµ‹æ•°æ®é›†
    """
    
    def __init__(self, data_dir: str, sequence_length: int = 180, predict_relative: bool = True):
        self.processor = SequenceProcessor(sequence_length, predict_relative)
        self.data = []
        
        print(f"åŠ è½½æ•°æ®ç›®å½•: {data_dir}")
        
        # åŠ è½½æ‰€æœ‰æ¸…æ´—åçš„æ•°æ®æ–‡ä»¶
        for data_file in glob.glob(f"{data_dir}/**/*.xlsx", recursive=True):
            try:
                df = pd.read_excel(data_file)
                print(f"å¤„ç†æ–‡ä»¶: {os.path.basename(data_file)}, æ•°æ®é•¿åº¦: {len(df)}")
                
                # åˆ›å»ºè®­ç»ƒåºåˆ—
                sequences = self.processor.create_training_sequences(df)
                self.data.extend(sequences)
                
                print(f"  ç”Ÿæˆåºåˆ—æ•°: {len(sequences)}")

            except Exception as e:
                print(f"å¤„ç†æ–‡ä»¶å¤±è´¥: {data_file}, é”™è¯¯: {e}")

        print(f"æ€»åºåˆ—æ•°: {len(self.data)}")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        input_seq, target_prices = self.data[idx]
        return torch.FloatTensor(input_seq), torch.FloatTensor(target_prices)




def get_price_median_from_features(feature_vector: np.ndarray) -> float:
    """
    ä»ç‰¹å¾å‘é‡ä¸­ç›´æ¥æå–ä»·æ ¼åŸºå‡†

    Args:
        feature_vector: [180, 20] ç‰¹å¾çŸ©é˜µ

    Returns:
        price_median: ä»·æ ¼åŸºå‡†å€¼
    """
    # price_median åœ¨ç¬¬13åˆ—ï¼ˆç´¢å¼•12ï¼‰- æŒ‰ç‰¹å¾åˆ—é¡ºåº
    # ç‰¹å¾é¡ºåºï¼šæœˆæ—¥æ˜ŸæœŸ(3) + ä»·æ ¼(4) + ä»·æ ¼å˜åŒ–(2) + æˆäº¤é‡(2) + é‡‘é¢(2) + æˆäº¤æ¬¡æ•°(1) + æ¢æ‰‹%(1) + price_median(1) + é‡‘è(4)
    # æ‰€ä»¥ price_median åœ¨ç´¢å¼• 3+4+2+2+2+1+1 = 15
    price_median = feature_vector[0, 15]  # æ‰€æœ‰æ—¶é—´æ­¥çš„ä»·æ ¼åŸºå‡†éƒ½ç›¸åŒï¼Œå–ç¬¬ä¸€ä¸ªå³å¯
    return float(price_median)


def convert_relative_to_absolute(relative_predictions: np.ndarray, price_median: float) -> np.ndarray:
    """
    å°†ç›¸å¯¹å€¼é¢„æµ‹è½¬æ¢ä¸ºç»å¯¹ä»·æ ¼ï¼ˆä»…åœ¨predict_relative=Trueæ—¶ä½¿ç”¨ï¼‰

    Args:
        relative_predictions: ç›¸å¯¹å€¼é¢„æµ‹ [batch_size, 10] æˆ– [10]
        price_median: ä»·æ ¼åŸºå‡†

    Returns:
        absolute_prices: ç»å¯¹ä»·æ ¼ [batch_size, 10] æˆ– [10]
    """
    return relative_predictions * price_median


def predict_stock_price(model, stock_file: str, processor: SequenceProcessor = None):
    """
    å¯¹å•åªè‚¡ç¥¨è¿›è¡Œä»·æ ¼é¢„æµ‹
    
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        stock_file: è‚¡ç¥¨æ•°æ®æ–‡ä»¶è·¯å¾„
        processor: åºåˆ—å¤„ç†å™¨
        
    Returns:
        predictions: [10] æœªæ¥10ä¸ªæ—¶é—´ç‚¹çš„é¢„æµ‹å€¼
    """
    if processor is None:
        processor = SequenceProcessor()
    
    # åŠ è½½æ•°æ®
    df = pd.read_excel(stock_file)
    
    # åˆ›å»ºé¢„æµ‹åºåˆ—
    input_seq = processor.create_prediction_sequence(df)
    input_tensor = torch.FloatTensor(input_seq).unsqueeze(0)  # [1, 180, 20]
    
    # é¢„æµ‹
    model.eval()
    with torch.no_grad():
        outputs = model(input_tensor)
        if isinstance(outputs, dict):
            predictions = outputs['price_predictions']  # [1, 10]
        else:
            predictions = outputs  # [1, 10]
    
    return predictions.squeeze().cpu().numpy()  # [10]


def validate_sequence_processing(cleaned_data: pd.DataFrame, processor: SequenceProcessor = None):
    """
    éªŒè¯åºåˆ—å¤„ç†æ˜¯å¦é¿å…äº†æ•°æ®æ³„éœ²
    """
    if processor is None:
        processor = SequenceProcessor()

    print("ğŸ” éªŒè¯åºåˆ—å¤„ç†...")

    # æ£€æŸ¥ä¸åŒåºåˆ—çš„ä»·æ ¼åŸºå‡†
    if len(cleaned_data) >= 280:
        sequence1 = cleaned_data.iloc[0:180]
        sequence2 = cleaned_data.iloc[100:280]

        features1 = processor.process_sequence_features(sequence1)
        features2 = processor.process_sequence_features(sequence2)

        # è®¡ç®—ä»·æ ¼åŸºå‡†
        median1 = np.median(sequence1[['å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜']].values.flatten())
        median2 = np.median(sequence2[['å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜']].values.flatten())

        print(f"åºåˆ—1ä»·æ ¼åŸºå‡†: {median1:.2f}")
        print(f"åºåˆ—2ä»·æ ¼åŸºå‡†: {median2:.2f}")
        print(f"åŸºå‡†å·®å¼‚: {abs(median1 - median2):.2f}")

        # æ£€æŸ¥ç‰¹å¾èŒƒå›´ï¼ˆä¸åº”è¯¥è¢«è£å‰ªï¼‰
        print(f"åºåˆ—1å¤§å•æ´»è·ƒåº¦èŒƒå›´: [{features1['big_order_activity'].min():.3f}, {features1['big_order_activity'].max():.3f}]")
        print(f"åºåˆ—2å¤§å•æ´»è·ƒåº¦èŒƒå›´: [{features2['big_order_activity'].min():.3f}, {features2['big_order_activity'].max():.3f}]")

        # æ£€æŸ¥æ˜¯å¦æœ‰NaNå€¼
        feature_vector1 = processor.build_feature_vector(sequence1, features1)
        feature_vector2 = processor.build_feature_vector(sequence2, features2)

        has_nan1 = np.isnan(feature_vector1).any()
        has_nan2 = np.isnan(feature_vector2).any()

        print(f"åºåˆ—1åŒ…å«NaN: {has_nan1}")
        print(f"åºåˆ—2åŒ…å«NaN: {has_nan2}")

        if not has_nan1 and not has_nan2:
            print("âœ… éªŒè¯é€šè¿‡ï¼šæ— æ•°æ®æ³„éœ²ï¼Œæ— NaNå€¼")
        else:
            print("âŒ éªŒè¯å¤±è´¥ï¼šå­˜åœ¨NaNå€¼")
    else:
        print("âŒ æ•°æ®é•¿åº¦ä¸è¶³ï¼Œæ— æ³•éªŒè¯")


def check_data_quality(sequences: List[Tuple[np.ndarray, np.ndarray]]):
    """
    æ£€æŸ¥å¤„ç†åæ•°æ®çš„è´¨é‡
    """
    print("ğŸ“Š æ•°æ®è´¨é‡æ£€æŸ¥...")
    print(f"åºåˆ—æ•°é‡: {len(sequences)}")

    if len(sequences) > 0:
        input_seq, target_seq = sequences[0]
        print(f"è¾“å…¥åºåˆ—å½¢çŠ¶: {input_seq.shape}")  # åº”è¯¥æ˜¯ (180, 20)
        print(f"ç›®æ ‡åºåˆ—å½¢çŠ¶: {target_seq.shape}")  # åº”è¯¥æ˜¯ (10,)

        # æ£€æŸ¥æ˜¯å¦æœ‰NaNå€¼
        all_inputs = np.array([seq[0] for seq in sequences])
        all_targets = np.array([seq[1] for seq in sequences])

        has_nan_inputs = np.isnan(all_inputs).any()
        has_nan_targets = np.isnan(all_targets).any()

        print(f"è¾“å…¥åŒ…å«NaNå€¼: {has_nan_inputs}")
        print(f"ç›®æ ‡åŒ…å«NaNå€¼: {has_nan_targets}")

        # æ£€æŸ¥æ•°å€¼èŒƒå›´
        feature_names = [
            'æœˆ', 'æ—¥', 'æ˜ŸæœŸ', 'open_rel', 'high_rel', 'low_rel', 'close_rel',
            'æ¶¨å¹…', 'æŒ¯å¹…', 'volume_rel', 'volume_change', 'amount_rel', 'amount_change',
            'æ¢æ‰‹%', 'æˆäº¤æ¬¡æ•°', 'big_order_activity', 'chip_concentration',
            'market_sentiment', 'price_volume_sync'
        ]

        print("\nç‰¹å¾æ•°å€¼èŒƒå›´:")
        for i, feature_name in enumerate(feature_names):
            feature_values = all_inputs[:, :, i].flatten()
            print(f"  {feature_name}: [{feature_values.min():.3f}, {feature_values.max():.3f}]")

        print(f"\nç›®æ ‡å€¼èŒƒå›´: [{all_targets.min():.3f}, {all_targets.max():.3f}]")


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
if __name__ == "__main__":
    print("ğŸš€ åºåˆ—å¤„ç†å™¨æµ‹è¯•")

    # æµ‹è¯•å•ä¸ªæ–‡ä»¶
    test_file = "processed_data_2025-07-30/è‚¡ç¥¨æ•°æ®/ç™½é…’/èŒ…å°.xlsx"
    if os.path.exists(test_file):
        print(f"\nğŸ“ æµ‹è¯•æ–‡ä»¶: {test_file}")

        # åŠ è½½æ•°æ®
        df = pd.read_excel(test_file)
        print(f"æ•°æ®å½¢çŠ¶: {df.shape}")

        # åˆ›å»ºå¤„ç†å™¨
        processor = SequenceProcessor()

        # éªŒè¯åºåˆ—å¤„ç†
        validate_sequence_processing(df, processor)

        # åˆ›å»ºè®­ç»ƒåºåˆ—
        print(f"\nğŸ”„ åˆ›å»ºè®­ç»ƒåºåˆ—...")
        sequences = processor.create_training_sequences(df)
        print(f"ç”Ÿæˆåºåˆ—æ•°: {len(sequences)}")

        # æ£€æŸ¥æ•°æ®è´¨é‡
        if len(sequences) > 0:
            check_data_quality(sequences[:100])  # æ£€æŸ¥å‰100ä¸ªåºåˆ—

        # æµ‹è¯•é¢„æµ‹åºåˆ—åˆ›å»º
        print(f"\nğŸ”® æµ‹è¯•é¢„æµ‹åºåˆ—åˆ›å»º...")
        try:
            pred_seq = processor.create_prediction_sequence(df)
            print(f"é¢„æµ‹åºåˆ—å½¢çŠ¶: {pred_seq.shape}")
            print("âœ… é¢„æµ‹åºåˆ—åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            print(f"âŒ é¢„æµ‹åºåˆ—åˆ›å»ºå¤±è´¥: {e}")

    else:
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")

        # åˆ›å»ºå®Œæ•´æ•°æ®é›†æµ‹è¯•
        data_dir = "processed_data_2025-07-30/è‚¡ç¥¨æ•°æ®"
        if os.path.exists(data_dir):
            print(f"\nğŸ“‚ æµ‹è¯•æ•°æ®ç›®å½•: {data_dir}")
            dataset = PriceDataset(data_dir)

            if len(dataset) > 0:
                # åˆ›å»ºæ•°æ®åŠ è½½å™¨
                dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

                # æ£€æŸ¥æ•°æ®
                for batch_idx, (inputs, targets) in enumerate(dataloader):
                    print(f"Batch {batch_idx}: inputs {inputs.shape}, targets {targets.shape}")
                    print(f"  è¾“å…¥èŒƒå›´: [{inputs.min():.3f}, {inputs.max():.3f}]")
                    print(f"  ç›®æ ‡èŒƒå›´: [{targets.min():.3f}, {targets.max():.3f}]")
                    if batch_idx == 0:
                        break
            else:
                print("âŒ æ•°æ®é›†ä¸ºç©º")
        else:
            print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
