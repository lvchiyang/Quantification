ä¸‹é¢ç»™å‡ºä¸€ä»½ä» 0 åˆ° 1 çš„ã€Œé€æ­¥æ‹†è§£ã€æ‰‹å†Œï¼Œå¸®åŠ©ä½ æœ€ç»ˆå¾—åˆ°ä¸€ä¸ªå¯è¿è¡Œçš„ã€**Decoder-only**ã€**Pre-RMSNorm + MLA + RoPE + SwiGLU + ä¸‹ä¸€ä¸ª token é¢„æµ‹** çš„ Transformer æ¨¡å‹ã€‚  
æ•´å¥—æµç¨‹æ‹†æˆ 8 æ­¥ï¼Œæ¯æ­¥éƒ½ç»™å‡ºã€Œè¦åšä»€ä¹ˆã€ã€Œæ€ä¹ˆåšã€ã€Œå…³é”®ä»£ç æ®µã€ä¸‰ä¸ªå­é¡¹ã€‚ä½ æ—¢å¯ä»¥æŒ‰é¡ºåºä¸€æ¬¡æ€§è·‘é€šï¼Œä¹Ÿå¯ä»¥æŒ‰éœ€è£å‰ªã€‚

---

### ğŸ“Œ ç¬¬ 0 æ­¥ï¼šç¯å¢ƒå‡†å¤‡
**åšä»€ä¹ˆ**  
- Python â‰¥ 3.8  
- PyTorch â‰¥ 2.1ï¼ˆæ”¯æŒ `torch.compile`ï¼‰  
- å¯é€‰ï¼š`flash-attn`ï¼ˆåŠ é€Ÿ MLAï¼‰ï¼Œ`einops`ï¼ˆå¼ é‡é‡æ’ï¼‰

```bash
pip install torch einops transformers datasets
# è‹¥ GPU â‰¥ A100/H100 å¯è£… flash-attn
pip install flash-attn --no-build-isolation
```

---

### ğŸ“Œ ç¬¬ 1 æ­¥ï¼šå®šä¹‰è¶…å‚æ•°é…ç½®
ç”¨ä¸€ä¸ª `dataclass` é›†ä¸­ç®¡ç†æ‰€æœ‰è¶…å‚ï¼Œé¿å…é­”æ³•æ•°å­—ã€‚

```python
from dataclasses import dataclass

@dataclass
class ModelArgs:
    d_model: int = 1024           # éšè—ç»´åº¦ h
    n_layers: int = 24            # Transformer å±‚æ•°
    n_heads: int = 16             # æŸ¥è¯¢å¤´æ•°
    kv_lora_rank: int = 512       # MLA ä¸­ K/V å‹ç¼©ç»´åº¦
    qk_rope_head_dim: int = 64    # RoPE ç»´åº¦
    qk_nope_head_dim: int = 128   # é RoPE æŸ¥è¯¢/é”®ç»´åº¦
    v_head_dim: int = 128         # å€¼å¤´ç»´åº¦
    intermediate_size: int = 2816 # SwiGLU ä¸­é—´ç»´åº¦ â‰ˆ 2/3 * 4h
    vocab_size: int = 32000
    max_seq_len: int = 2048
    dropout: float = 0.0
```

---

### ğŸ“Œ ç¬¬ 2 æ­¥ï¼šTokenizerï¼ˆç›´æ¥å¤ç”¨å·²æœ‰è¯è¡¨ï¼‰
```python
from transformers import AutoTokenizer
tok = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf", use_fast=True)
tok.pad_token = tok.eos_token
```

---

### ğŸ“Œ ç¬¬ 3 æ­¥ï¼šå…¬å…±å·¥å…·
- **RMSNorm**ï¼ˆPre-normalizationï¼‰
```python
import torch.nn as nn
class RMSNorm(nn.Module):
    def __init__(self, dim, eps=1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(dim))
        self.eps = eps
    def forward(self, x):
        norm = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        return self.weight * norm
```

- **RoPE**ï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰
```python
import torch
def precompute_freqs_cis(dim, end, theta=1e4):
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))
    t = torch.arange(end, device=freqs.device)
    freqs = torch.outer(t, freqs).float()
    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)
    return freqs_cis
def apply_rotary_emb(xq, xk, freqs_cis):
    # xq, xk: [bsz, seqlen, n_heads, head_dim]
    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))
    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))
    freqs_cis = freqs_cis[:xq_.shape[1]]
    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)
    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)
    return xq_out.type_as(xq), xk_out.type_as(xk)
```

---

### ğŸ“Œ ç¬¬ 4 æ­¥ï¼šMLAï¼ˆMulti-Head Latent Attentionï¼‰
MLA çš„æ ¸å¿ƒï¼š  
1. å°† K/V é€šè¿‡ **ä½ç§©æŠ•å½±** å‹ç¼©æˆ `kv_lora_rank` ç»´ latentï¼›  
2. å¯¹ Q æ‹†å‡º **RoPE éƒ¨åˆ†** å’Œ **é RoPE éƒ¨åˆ†**ï¼›  
3. è®¡ç®—æ³¨æ„åŠ›æ—¶ï¼Œå…ˆè§£å‹ K/Vï¼Œå†æ‹¼æ¥ã€‚

```python
from einops import rearrange
class MLA(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.n_heads = args.n_heads
        self.qk_nope_dim = args.qk_nope_head_dim
        self.qk_rope_dim = args.qk_rope_head_dim
        self.v_dim = args.v_head_dim
        self.kv_lora_rank = args.kv_lora_rank

        # å‹ç¼©åçš„ K/V latent
        self.kv_compress = nn.Linear(args.d_model, self.kv_lora_rank, bias=False)
        self.kv_norm = RMSNorm(self.kv_lora_rank)
        # è§£å‹æŠ•å½±
        self.k_up = nn.Linear(self.kv_lora_rank, self.n_heads * (self.qk_nope_dim + self.qk_rope_dim), bias=False)
        self.v_up = nn.Linear(self.kv_lora_rank, self.n_heads * self.v_dim, bias=False)

        # æŸ¥è¯¢æŠ•å½±ï¼ˆæ‹†åˆ†ä¸º RoPE ä¸é RoPEï¼‰
        self.q_nope = nn.Linear(args.d_model, self.n_heads * self.qk_nope_dim, bias=False)
        self.q_rope = nn.Linear(args.d_model, self.n_heads * self.qk_rope_dim, bias=False)

        self.out_proj = nn.Linear(self.n_heads * self.v_dim, args.d_model, bias=False)

    def forward(self, x, freqs_cis, mask=None):
        bsz, seqlen, _ = x.shape

        # 1) å‹ç¼© K/V åˆ° latent
        kv_latent = self.kv_norm(self.kv_compress(x))                        # [bsz, seqlen, kv_lora_rank]

        # 2) è§£å‹
        kv = self.k_up(kv_latent).view(bsz, seqlen, self.n_heads, -1)
        k_nope, k_rope = kv.split([self.qk_nope_dim, self.qk_rope_dim], dim=-1)

        v = self.v_up(kv_latent).view(bsz, seqlen, self.n_heads, self.v_dim)

        # 3) æŸ¥è¯¢
        q_nope = self.q_nope(x).view(bsz, seqlen, self.n_heads, self.qk_nope_dim)
        q_rope = self.q_rope(x).view(bsz, seqlen, self.n_heads, self.qk_rope_dim)
        q_rope, k_rope = apply_rotary_emb(q_rope, k_rope, freqs_cis)

        # æ‹¼æ¥
        q = torch.cat([q_nope, q_rope], dim=-1)   # [bsz, seqlen, n_heads, qk_dim]
        k = torch.cat([k_nope, k_rope], dim=-1)

        # 4) æ³¨æ„åŠ›è®¡ç®—
        scores = torch.einsum("bshd,bthd->bhst", q, k) / (q.shape[-1] ** 0.5)
        if mask is not None:
            scores = scores + mask
        probs = torch.softmax(scores, dim=-1)
        out = torch.einsum("bhst,bthd->bshd", probs, v)
        out = rearrange(out, "b s h d -> b s (h d)")
        return self.out_proj(out)
```

---

### ğŸ“Œ ç¬¬ 5 æ­¥ï¼šSwiGLU FFN
```python
class SwiGLU(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        hidden = args.intermediate_size
        self.w1 = nn.Linear(args.d_model, hidden, bias=False)
        self.w2 = nn.Linear(hidden, args.d_model, bias=False)
        self.w3 = nn.Linear(args.d_model, hidden, bias=False)
    def forward(self, x):
        return self.w2(nn.functional.silu(self.w1(x)) * self.w3(x))
```

---

### ğŸ“Œ ç¬¬ 6 æ­¥ï¼šTransformer Blockï¼ˆPre-RMSNormï¼‰
```python
class TransformerBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.attn_norm = RMSNorm(args.d_model)
        self.attn = MLA(args)
        self.ffn_norm  = RMSNorm(args.d_model)
        self.ffn = SwiGLU(args)
    def forward(self, x, freqs_cis, mask):
        x = x + self.attn(self.attn_norm(x), freqs_cis, mask)
        x = x + self.ffn(self.ffn_norm(x))
        return x
```

---

### ğŸ“Œ ç¬¬ 7 æ­¥ï¼šå®Œæ•´ Decoder æ¨¡å‹
```python
class Transformer(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.tok_embed = nn.Embedding(args.vocab_size, args.d_model)
        self.layers = nn.ModuleList([TransformerBlock(args) for _ in range(args.n_layers)])
        self.norm = RMSNorm(args.d_model)
        self.lm_head = nn.Linear(args.d_model, args.vocab_size, bias=False)
        # é¢„è®¡ç®— RoPE
        self.register_buffer("freqs_cis", precompute_freqs_cis(
            args.qk_rope_head_dim, args.max_seq_len * 2))

    def forward(self, input_ids, labels=None):
        bsz, seqlen = input_ids.shape
        h = self.tok_embed(input_ids)
        freqs_cis = self.freqs_cis[:seqlen]

        mask = torch.full((seqlen, seqlen), float("-inf"), device=h.device)
        mask = torch.triu(mask, diagonal=1)

        for layer in self.layers:
            h = layer(h, freqs_cis, mask)
        h = self.norm(h)
        logits = self.lm_head(h)

        loss = None
        if labels is not None:
            loss = nn.functional.cross_entropy(
                logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=-100)
        return logits, loss
```

---

### ğŸ“Œ ç¬¬ 8 æ­¥ï¼šè®­ç»ƒè„šæœ¬ï¼ˆæœ€å°å¯è¿è¡Œï¼‰
```python
from datasets import load_dataset
from torch.utils.data import DataLoader

# 1) æ•°æ®ï¼šéšä¾¿æ‹¿ä¸€ä¸ªå¼€æºè¯­æ–™åš next token
ds = load_dataset("roneneldan/TinyStories", split="train")
def tokenize(batch):
    return tok(batch["text"], truncation=True, max_length=512, padding="max_length")
ds = ds.map(tokenize, batched=True, remove_columns=ds.column_names)
dl = DataLoader(ds, batch_size=16, shuffle=True)

# 2) æ¨¡å‹
args = ModelArgs()
model = Transformer(args).cuda()
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)

# 3) è®­ç»ƒå¾ªç¯
for epoch in range(3):
    for batch in dl:
        input_ids = torch.tensor(batch["input_ids"]).cuda()
        labels = input_ids.clone()
        logits, loss = model(input_ids, labels)
        loss.backward()
        optimizer.step(); optimizer.zero_grad()
        print(loss.item())
```

---

### âœ… éªŒè¯
è®­ç»ƒè‹¥å¹² step åï¼Œå¯ç”¨ `model.generate()` è‡ªå›å½’é‡‡æ ·ï¼š
```python
model.eval()
prompt = "Once upon a time"
ids = tok.encode(prompt, return_tensors="pt").cuda()
for _ in range(50):
    logits, _ = model(ids)
    next_id = logits[:, -1].argmax(-1, keepdim=True)
    ids = torch.cat([ids, next_id], dim=1)
print(tok.decode(ids[0]))
```

---

### ğŸ“¦ å°ç»“
| ç»„ä»¶        | å®ç°ä½ç½®     | å…³é”®è¦ç‚¹ |
|-------------|--------------|----------|
| Pre-RMSNorm | æ¯å±‚è¾“å…¥å‰   | å…ˆ norm åæ®‹å·® |
| MLA         | `MLA` ç±»     | KV å‹ç¼© latent + RoPE |
| RoPE        | `apply_rotary_emb` | æ—‹è½¬ä½ç½®ç¼–ç  |
| SwiGLU      | `SwiGLU` ç±»  | `silu(W1 x) * W3 x` |
| Next-token  | `lm_head` + CE loss | æ ‡å‡†è¯­è¨€æ¨¡å‹ç›®æ ‡ |

