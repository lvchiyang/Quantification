下面给出一份从 0 到 1 的「逐步拆解」手册，帮助你最终得到一个可运行的、**Decoder-only**、**Pre-RMSNorm + MLA + RoPE + SwiGLU + 下一个 token 预测** 的 Transformer 模型。  
整套流程拆成 8 步，每步都给出「要做什么」「怎么做」「关键代码段」三个子项。你既可以按顺序一次性跑通，也可以按需裁剪。

---

### 📌 第 0 步：环境准备
**做什么**  
- Python ≥ 3.8  
- PyTorch ≥ 2.1（支持 `torch.compile`）  
- 可选：`flash-attn`（加速 MLA），`einops`（张量重排）

```bash
pip install torch einops transformers datasets
# 若 GPU ≥ A100/H100 可装 flash-attn
pip install flash-attn --no-build-isolation
```

---

### 📌 第 1 步：定义超参数配置
用一个 `dataclass` 集中管理所有超参，避免魔法数字。

```python
from dataclasses import dataclass

@dataclass
class ModelArgs:
    d_model: int = 1024           # 隐藏维度 h
    n_layers: int = 24            # Transformer 层数
    n_heads: int = 16             # 查询头数
    kv_lora_rank: int = 512       # MLA 中 K/V 压缩维度
    qk_rope_head_dim: int = 64    # RoPE 维度
    qk_nope_head_dim: int = 128   # 非 RoPE 查询/键维度
    v_head_dim: int = 128         # 值头维度
    intermediate_size: int = 2816 # SwiGLU 中间维度 ≈ 2/3 * 4h
    vocab_size: int = 32000
    max_seq_len: int = 2048
    dropout: float = 0.0
```

---

### 📌 第 2 步：Tokenizer（直接复用已有词表）
```python
from transformers import AutoTokenizer
tok = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf", use_fast=True)
tok.pad_token = tok.eos_token
```

---

### 📌 第 3 步：公共工具
- **RMSNorm**（Pre-normalization）
```python
import torch.nn as nn
class RMSNorm(nn.Module):
    def __init__(self, dim, eps=1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(dim))
        self.eps = eps
    def forward(self, x):
        norm = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        return self.weight * norm
```

- **RoPE**（旋转位置编码）
```python
import torch
def precompute_freqs_cis(dim, end, theta=1e4):
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))
    t = torch.arange(end, device=freqs.device)
    freqs = torch.outer(t, freqs).float()
    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)
    return freqs_cis
def apply_rotary_emb(xq, xk, freqs_cis):
    # xq, xk: [bsz, seqlen, n_heads, head_dim]
    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))
    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))
    freqs_cis = freqs_cis[:xq_.shape[1]]
    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)
    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)
    return xq_out.type_as(xq), xk_out.type_as(xk)
```

---

### 📌 第 4 步：MLA（Multi-Head Latent Attention）
MLA 的核心：  
1. 将 K/V 通过 **低秩投影** 压缩成 `kv_lora_rank` 维 latent；  
2. 对 Q 拆出 **RoPE 部分** 和 **非 RoPE 部分**；  
3. 计算注意力时，先解压 K/V，再拼接。

```python
from einops import rearrange
class MLA(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.n_heads = args.n_heads
        self.qk_nope_dim = args.qk_nope_head_dim
        self.qk_rope_dim = args.qk_rope_head_dim
        self.v_dim = args.v_head_dim
        self.kv_lora_rank = args.kv_lora_rank

        # 压缩后的 K/V latent
        self.kv_compress = nn.Linear(args.d_model, self.kv_lora_rank, bias=False)
        self.kv_norm = RMSNorm(self.kv_lora_rank)
        # 解压投影
        self.k_up = nn.Linear(self.kv_lora_rank, self.n_heads * (self.qk_nope_dim + self.qk_rope_dim), bias=False)
        self.v_up = nn.Linear(self.kv_lora_rank, self.n_heads * self.v_dim, bias=False)

        # 查询投影（拆分为 RoPE 与非 RoPE）
        self.q_nope = nn.Linear(args.d_model, self.n_heads * self.qk_nope_dim, bias=False)
        self.q_rope = nn.Linear(args.d_model, self.n_heads * self.qk_rope_dim, bias=False)

        self.out_proj = nn.Linear(self.n_heads * self.v_dim, args.d_model, bias=False)

    def forward(self, x, freqs_cis, mask=None):
        bsz, seqlen, _ = x.shape

        # 1) 压缩 K/V 到 latent
        kv_latent = self.kv_norm(self.kv_compress(x))                        # [bsz, seqlen, kv_lora_rank]

        # 2) 解压
        kv = self.k_up(kv_latent).view(bsz, seqlen, self.n_heads, -1)
        k_nope, k_rope = kv.split([self.qk_nope_dim, self.qk_rope_dim], dim=-1)

        v = self.v_up(kv_latent).view(bsz, seqlen, self.n_heads, self.v_dim)

        # 3) 查询
        q_nope = self.q_nope(x).view(bsz, seqlen, self.n_heads, self.qk_nope_dim)
        q_rope = self.q_rope(x).view(bsz, seqlen, self.n_heads, self.qk_rope_dim)
        q_rope, k_rope = apply_rotary_emb(q_rope, k_rope, freqs_cis)

        # 拼接
        q = torch.cat([q_nope, q_rope], dim=-1)   # [bsz, seqlen, n_heads, qk_dim]
        k = torch.cat([k_nope, k_rope], dim=-1)

        # 4) 注意力计算
        scores = torch.einsum("bshd,bthd->bhst", q, k) / (q.shape[-1] ** 0.5)
        if mask is not None:
            scores = scores + mask
        probs = torch.softmax(scores, dim=-1)
        out = torch.einsum("bhst,bthd->bshd", probs, v)
        out = rearrange(out, "b s h d -> b s (h d)")
        return self.out_proj(out)
```

---

### 📌 第 5 步：SwiGLU FFN
```python
class SwiGLU(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        hidden = args.intermediate_size
        self.w1 = nn.Linear(args.d_model, hidden, bias=False)
        self.w2 = nn.Linear(hidden, args.d_model, bias=False)
        self.w3 = nn.Linear(args.d_model, hidden, bias=False)
    def forward(self, x):
        return self.w2(nn.functional.silu(self.w1(x)) * self.w3(x))
```

---

### 📌 第 6 步：Transformer Block（Pre-RMSNorm）
```python
class TransformerBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.attn_norm = RMSNorm(args.d_model)
        self.attn = MLA(args)
        self.ffn_norm  = RMSNorm(args.d_model)
        self.ffn = SwiGLU(args)
    def forward(self, x, freqs_cis, mask):
        x = x + self.attn(self.attn_norm(x), freqs_cis, mask)
        x = x + self.ffn(self.ffn_norm(x))
        return x
```

---

### 📌 第 7 步：完整 Decoder 模型
```python
class Transformer(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.tok_embed = nn.Embedding(args.vocab_size, args.d_model)
        self.layers = nn.ModuleList([TransformerBlock(args) for _ in range(args.n_layers)])
        self.norm = RMSNorm(args.d_model)
        self.lm_head = nn.Linear(args.d_model, args.vocab_size, bias=False)
        # 预计算 RoPE
        self.register_buffer("freqs_cis", precompute_freqs_cis(
            args.qk_rope_head_dim, args.max_seq_len * 2))

    def forward(self, input_ids, labels=None):
        bsz, seqlen = input_ids.shape
        h = self.tok_embed(input_ids)
        freqs_cis = self.freqs_cis[:seqlen]

        mask = torch.full((seqlen, seqlen), float("-inf"), device=h.device)
        mask = torch.triu(mask, diagonal=1)

        for layer in self.layers:
            h = layer(h, freqs_cis, mask)
        h = self.norm(h)
        logits = self.lm_head(h)

        loss = None
        if labels is not None:
            loss = nn.functional.cross_entropy(
                logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=-100)
        return logits, loss
```

---

### 📌 第 8 步：训练脚本（最小可运行）
```python
from datasets import load_dataset
from torch.utils.data import DataLoader

# 1) 数据：随便拿一个开源语料做 next token
ds = load_dataset("roneneldan/TinyStories", split="train")
def tokenize(batch):
    return tok(batch["text"], truncation=True, max_length=512, padding="max_length")
ds = ds.map(tokenize, batched=True, remove_columns=ds.column_names)
dl = DataLoader(ds, batch_size=16, shuffle=True)

# 2) 模型
args = ModelArgs()
model = Transformer(args).cuda()
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)

# 3) 训练循环
for epoch in range(3):
    for batch in dl:
        input_ids = torch.tensor(batch["input_ids"]).cuda()
        labels = input_ids.clone()
        logits, loss = model(input_ids, labels)
        loss.backward()
        optimizer.step(); optimizer.zero_grad()
        print(loss.item())
```

---

### ✅ 验证
训练若干 step 后，可用 `model.generate()` 自回归采样：
```python
model.eval()
prompt = "Once upon a time"
ids = tok.encode(prompt, return_tensors="pt").cuda()
for _ in range(50):
    logits, _ = model(ids)
    next_id = logits[:, -1].argmax(-1, keepdim=True)
    ids = torch.cat([ids, next_id], dim=1)
print(tok.decode(ids[0]))
```

---

### 📦 小结
| 组件        | 实现位置     | 关键要点 |
|-------------|--------------|----------|
| Pre-RMSNorm | 每层输入前   | 先 norm 后残差 |
| MLA         | `MLA` 类     | KV 压缩 latent + RoPE |
| RoPE        | `apply_rotary_emb` | 旋转位置编码 |
| SwiGLU      | `SwiGLU` 类  | `silu(W1 x) * W3 x` |
| Next-token  | `lm_head` + CE loss | 标准语言模型目标 |

