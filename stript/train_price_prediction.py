"""
ä»·æ ¼é¢„æµ‹ç½‘ç»œç‹¬ç«‹è®­ç»ƒè„šæœ¬
ä¸“é—¨è®­ç»ƒTransformeræ¶æ„çš„ä»·æ ¼é¢„æµ‹æ¨¡å‹
"""

import sys
import os
import glob
from datetime import datetime
sys.path.append('src')

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt

from price_prediction.config import PricePredictionConfigs
from price_prediction.price_transformer import PriceTransformer
from price_prediction.financial_losses import FinancialMultiLoss
# ä½¿ç”¨åºåˆ—å¤„ç†å™¨
sys.path.append('.')
from sequence_processor import PriceDataset


def find_latest_data_dir():
    """æŸ¥æ‰¾æœ€æ–°çš„æ•°æ®ç›®å½•"""
    pattern = "processed_data_*"
    data_dirs = glob.glob(pattern)

    if not data_dirs:
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•åˆ›å»ºä»Šå¤©çš„ç›®å½•å
        today = datetime.now().strftime("%Y-%m-%d")
        return f"processed_data_{today}"

    # è¿”å›æœ€æ–°çš„ç›®å½•
    return sorted(data_dirs)[-1]



def train_price_prediction_model(config):
    """è®­ç»ƒä»·æ ¼é¢„æµ‹æ¨¡å‹"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒä»·æ ¼é¢„æµ‹æ¨¡å‹")
    print("="*50)

    # 1. æŸ¥æ‰¾æ•°æ®ç›®å½•
    print("ğŸ“Š æŸ¥æ‰¾æ•°æ®ç›®å½•...")
    data_dir = find_latest_data_dir()
    data_path = os.path.join(data_dir, "è‚¡ç¥¨æ•°æ®")

    print(f"ä½¿ç”¨æ•°æ®ç›®å½•: {data_dir}")

    if not os.path.exists(data_path):
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_path}")
        print("è¯·ç¡®ä¿æ•°æ®å·²ç»å¤„ç†å®Œæˆï¼")
        return None

    # 2. ä½¿ç”¨åºåˆ—å¤„ç†å™¨åˆ›å»ºæ•°æ®é›†
    dataset = PriceDataset(data_path, sequence_length=config.sequence_length)

    if len(dataset) == 0:
        print("âŒ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ•°æ®")
        return None

    print(f"\nğŸ“ˆ è®­ç»ƒæ•°æ®ç»Ÿè®¡:")
    print(f"  æ€»åºåˆ—æ•°: {len(dataset)}")

    # æ£€æŸ¥æ•°æ®å½¢çŠ¶
    sample_input, sample_target = dataset[0]
    print(f"  è¾“å…¥å½¢çŠ¶: {sample_input.shape}")  # [180, 20]
    print(f"  ç›®æ ‡å½¢çŠ¶: {sample_target.shape}")  # [10]

    # 3. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True)
    
    # 5. åˆ›å»ºæ¨¡å‹
    print(f"\nğŸ—ï¸  åˆ›å»ºæ¨¡å‹...")
    model = PriceTransformer(config)
    
    # è®¡ç®—å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"  æ€»å‚æ•°æ•°: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    # 6. è®¾ç½®ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.AdamW(
        model.parameters(),
        lr=config.learning_rate,
        weight_decay=config.weight_decay
    )

    # æ ¹æ®é…ç½®é€‰æ‹©æŸå¤±å‡½æ•°
    if config.use_financial_loss:
        # ä½¿ç”¨é‡‘èä¸“ç”¨å¤šæŸå¤±å‡½æ•°ç»„åˆ
        print(f"ğŸ¯ ä½¿ç”¨é‡‘èä¸“ç”¨å¤šæŸå¤±å‡½æ•°:")
        print(f"  åŸºç¡€æŸå¤±ç±»å‹: {config.loss_type}")

        criterion = FinancialMultiLoss(
            base_loss_type=config.loss_type,
            use_direction_loss=config.use_direction_loss,
            use_trend_loss=config.use_trend_loss,
            use_temporal_weighting=config.use_temporal_weighting,
            use_ranking_loss=config.use_ranking_loss,
            use_volatility_loss=config.use_volatility_loss,

            # æŸå¤±æƒé‡é…ç½®
            base_weight=config.base_weight,
            direction_weight=config.direction_weight,
            trend_weight=config.trend_weight,
            ranking_weight=config.ranking_weight,
            volatility_weight=config.volatility_weight
        )

        enabled_components = []
        if config.use_direction_loss:
            enabled_components.append(f"æ–¹å‘æŸå¤±({config.direction_weight})")
        if config.use_trend_loss:
            enabled_components.append(f"è¶‹åŠ¿æŸå¤±({config.trend_weight})")
        if config.use_temporal_weighting:
            enabled_components.append("æ—¶é—´åŠ æƒ")
        if config.use_ranking_loss:
            enabled_components.append(f"æ’åºæŸå¤±({config.ranking_weight})")
        if config.use_volatility_loss:
            enabled_components.append(f"æ³¢åŠ¨ç‡æŸå¤±({config.volatility_weight})")

        print(f"  å¯ç”¨ç»„ä»¶: {' + '.join(enabled_components)}")
        use_multi_loss = True

    else:
        # ä½¿ç”¨ä¼ ç»Ÿå•ä¸€æŸå¤±å‡½æ•°
        print(f"ğŸ¯ ä½¿ç”¨ä¼ ç»ŸæŸå¤±å‡½æ•°: {config.loss_type}")

        if config.loss_type == "mse":
            criterion = nn.MSELoss()
        elif config.loss_type == "mae":
            criterion = nn.L1Loss()
        elif config.loss_type == "huber":
            criterion = nn.HuberLoss(delta=config.huber_delta)
        else:
            criterion = nn.MSELoss()

        use_multi_loss = False
    
    # 7. è®­ç»ƒå¾ªç¯
    print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ...")
    train_losses = []
    best_loss = float('inf')
    patience_counter = 0
    
    for epoch in range(config.max_epochs):
        model.train()
        epoch_losses = []
        
        # è®­ç»ƒä¸€ä¸ªepoch
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{config.max_epochs}")
        for features, targets in pbar:
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            outputs = model(features)
            price_predictions = outputs['price_predictions']
            
            # è®¡ç®—æŸå¤±
            if use_multi_loss:
                # é‡‘èä¸“ç”¨å¤šæŸå¤±å‡½æ•°
                loss_dict = criterion(price_predictions, targets)
                total_loss = loss_dict['loss']  # ä¸»æŸå¤±ç”¨äºåå‘ä¼ æ’­

                # æ˜¾ç¤ºè¯¦ç»†æŸå¤±ä¿¡æ¯
                pbar.set_postfix({
                    'total': f'{total_loss.item():.4f}',
                    'base': f'{loss_dict["base_loss"]:.4f}',
                    'dir': f'{loss_dict["direction_loss"]:.4f}',
                    'trend': f'{loss_dict["trend_loss"]:.4f}',
                    'dir_acc': f'{loss_dict["direction_accuracy"]:.2%}'
                })
            else:
                # ä¼ ç»Ÿå•ä¸€æŸå¤±å‡½æ•°
                total_loss = criterion(price_predictions, targets)

                # æ˜¾ç¤ºç®€å•æŸå¤±ä¿¡æ¯
                pbar.set_postfix({'loss': f'{total_loss.item():.6f}'})

            # åå‘ä¼ æ’­
            total_loss.backward()

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()

            # è®°å½•æŸå¤±
            epoch_losses.append(total_loss.item())
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_loss = np.mean(epoch_losses)
        train_losses.append(avg_loss)
        
        print(f"Epoch {epoch+1}: å¹³å‡æ€»æŸå¤± = {avg_loss:.6f}")
        
        # æ—©åœæ£€æŸ¥
        if avg_loss < best_loss:
            best_loss = avg_loss
            patience_counter = 0
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            os.makedirs(config.save_dir, exist_ok=True)
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': best_loss,
                'config': config
            }, os.path.join(config.save_dir, 'best_price_model.pth'))
            
        else:
            patience_counter += 1
            if patience_counter >= config.early_stopping_patience:
                print(f"æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch+1} è½®åœæ­¢è®­ç»ƒ")
                break
        
        # å®šæœŸä¿å­˜
        if (epoch + 1) % config.save_every_n_epochs == 0:
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_loss,
                'config': config
            }, os.path.join(config.save_dir, f'price_model_epoch_{epoch+1}.pth'))
    
    # 8. ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='è®­ç»ƒæŸå¤±')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('ä»·æ ¼é¢„æµ‹æ¨¡å‹è®­ç»ƒæ›²çº¿')
    plt.legend()
    plt.grid(True)
    plt.savefig(os.path.join(config.save_dir, 'training_curve.png'))
    plt.show()
    
    print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³æŸå¤±: {best_loss:.6f}")
    print(f"æ¨¡å‹ä¿å­˜åœ¨: {config.save_dir}")
    
    return model, train_losses

def main():
    """ä¸»å‡½æ•°"""
    # é€‰æ‹©é…ç½®
    print("é€‰æ‹©æ¨¡å‹é…ç½®:")
    print("1. tiny - è½»é‡çº§æµ‹è¯•")
    print("2. small - å°å‹æ¨¡å‹")
    print("3. base - åŸºç¡€æ¨¡å‹ï¼ˆæ¨èï¼‰")
    print("4. large - å¤§å‹æ¨¡å‹")
    print("5. long_sequence - é•¿åºåˆ—ä¸“ç”¨")
    
    choice = input("è¯·é€‰æ‹©é…ç½® (1-5, é»˜è®¤3): ").strip() or "3"
    
    config_map = {
        "1": PricePredictionConfigs.tiny(),
        "2": PricePredictionConfigs.small(),
        "3": PricePredictionConfigs.base(),
        "4": PricePredictionConfigs.large(),
        "5": PricePredictionConfigs.for_long_sequence()
    }
    
    config = config_map.get(choice, PricePredictionConfigs.base())
    
    # å¼€å§‹è®­ç»ƒ
    try:
        result = train_price_prediction_model(config)

        if result is not None:
            model, train_losses = result
            print("\nğŸ‰ ä»·æ ¼é¢„æµ‹æ¨¡å‹è®­ç»ƒæˆåŠŸ!")
            print("ç°åœ¨å¯ä»¥ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œä»·æ ¼é¢„æµ‹äº†ã€‚")
        else:
            print("\nâŒ è®­ç»ƒå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®å’Œé…ç½®")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
