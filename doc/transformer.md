# Transformer æ¶æ„æ–‡æ¡£

ä¸“é—¨ä¸ºé‡‘èæ—¶åºé¢„æµ‹è®¾è®¡çš„ Transformer æ¶æ„ï¼ŒåŒ…å« TransformerBlock å’Œç›¸å…³ç»„ä»¶çš„è¯¦ç»†è¯´æ˜ã€‚

## ğŸ“‹ ç›®å½•

- [æ¶æ„æ¦‚è¿°](#æ¶æ„æ¦‚è¿°)
- [TransformerBlock](#transformerblock)
- [æ ¸å¿ƒç»„ä»¶](#æ ¸å¿ƒç»„ä»¶)
- [ä½¿ç”¨æ–¹æ³•](#ä½¿ç”¨æ–¹æ³•)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)

---

## ğŸ—ï¸ æ¶æ„æ¦‚è¿°

### æ•´ä½“è®¾è®¡ç†å¿µ

æœ¬ Transformer æ¶æ„ä¸“é—¨é’ˆå¯¹é‡‘èæ—¶åºé¢„æµ‹ä»»åŠ¡ä¼˜åŒ–ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

1. **Multi-Head Latent Attention (MLA)**ï¼šé«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶
2. **RoPE ä½ç½®ç¼–ç **ï¼šæ—‹è½¬ä½ç½®ç¼–ç ï¼Œé€‚åˆé•¿åºåˆ—
3. **SwiGLU å‰é¦ˆç½‘ç»œ**ï¼šç°ä»£åŒ–çš„å‰é¦ˆç½‘ç»œè®¾è®¡
4. **Pre-RMSNorm**ï¼šæ›´ç¨³å®šçš„å½’ä¸€åŒ–æ–¹å¼
5. **é‡‘èç‰¹å¾åµŒå…¥**ï¼šä¸“é—¨çš„ç‰¹å¾åˆ†ç»„åµŒå…¥

### æ•°æ®æµ

```
è¾“å…¥: [batch, 180, 20] é‡‘èç‰¹å¾
  â†“
FinancialEmbeddingLayer: åˆ†ç»„åµŒå…¥ + æ‰¹æ ‡å‡†åŒ–
  â†“
[batch, 180, d_model] åµŒå…¥ç‰¹å¾
  â†“
TransformerBlock Ã— n_layers:
  - RMSNorm â†’ MLA + RoPE â†’ æ®‹å·®è¿æ¥
  - RMSNorm â†’ SwiGLU FFN â†’ æ®‹å·®è¿æ¥
  â†“
[batch, 180, d_model] ç¼–ç ç‰¹å¾
  â†“
å–æœ€åæ—¶é—´æ­¥: [batch, d_model]
  â†“
ä»·æ ¼é¢„æµ‹å¤´: [batch, 10] æœªæ¥10ä¸ªæ—¶é—´ç‚¹é¢„æµ‹
```

---

## ğŸ§© TransformerBlock

### è®¾è®¡åŸç†

TransformerBlock æ˜¯ Transformer çš„æ ¸å¿ƒç»„ä»¶ï¼Œé‡‡ç”¨ Pre-RMSNorm ç»“æ„ï¼š

```python
class TransformerBlock(nn.Module):
    """
    ç»“æ„ï¼š
    x -> RMSNorm -> MLA -> Add -> RMSNorm -> FFN -> Add
    """
    
    def __init__(self, args, layer_idx: int = 0):
        super().__init__()
        
        # æ³¨æ„åŠ›å±‚
        self.attn_norm = RMSNorm(args.d_model)
        self.attn = MultiHeadLatentAttention(args)
        
        # å‰é¦ˆç½‘ç»œå±‚
        self.ffn_norm = RMSNorm(args.d_model)
        self.ffn = get_ffn(args, ffn_type="swiglu")
```

### å‰å‘ä¼ æ’­

```python
def forward(self, x, freqs_cis, attn_mask=None, is_causal=False):
    # Pre-RMSNorm + MLA + æ®‹å·®è¿æ¥
    attn_input = self.attn_norm(x)
    attn_output = self.attn(attn_input, freqs_cis, attn_mask, is_causal)
    x = x + attn_output
    
    # Pre-RMSNorm + FFN + æ®‹å·®è¿æ¥
    ffn_input = self.ffn_norm(x)
    ffn_output = self.ffn(ffn_input)
    x = x + ffn_output
    
    return x
```

### å…³é”®ç‰¹æ€§

1. **Pre-RMSNorm**ï¼š
   - åœ¨æ³¨æ„åŠ›å’ŒFFNä¹‹å‰è¿›è¡Œå½’ä¸€åŒ–
   - æ¯” Post-Norm æ›´ç¨³å®šï¼Œè®­ç»ƒæ›´å®¹æ˜“

2. **æ®‹å·®è¿æ¥**ï¼š
   - ç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
   - å…è®¸æ›´æ·±çš„ç½‘ç»œç»“æ„

3. **å±‚ç´¢å¼•**ï¼š
   - æ”¯æŒå±‚ç‰¹å®šçš„é…ç½®
   - ä¾¿äºåˆ†æä¸åŒå±‚çš„ä½œç”¨

---

## ğŸ”§ æ ¸å¿ƒç»„ä»¶

### 1. Multi-Head Latent Attention (MLA)

**ä¼ ç»Ÿæ³¨æ„åŠ› vs MLA**ï¼š

```python
# ä¼ ç»Ÿæ³¨æ„åŠ›
Traditional: Q@K^T@V  # O(nÂ²d) å¤æ‚åº¦

# MLA
MLA: Q@(W_kv@X)^T@(W_kv@X)  # O(nd) å¤æ‚åº¦ï¼Œd<<næ—¶é«˜æ•ˆ

# K/Vå‹ç¼©ç¤ºä¾‹
original_kv_dim = 1024    # åŸå§‹ç»´åº¦
compressed_dim = 256      # å‹ç¼©åç»´åº¦
compression_ratio = 4     # å‹ç¼©æ¯”ä¾‹
```

**MLAå®ç°ç»†èŠ‚**ï¼š

```python
class MultiHeadLatentAttention(nn.Module):
    def __init__(self, args):
        # æŸ¥è¯¢/é”®å¤´ç»´åº¦ï¼Œä½¿ç”¨æ ‡å‡†çš„d_model/n_heads
        self.qk_head_dim = args.d_model // args.n_heads
        
        # ç¡®ä¿å¤´ç»´åº¦æ˜¯å¶æ•°ï¼ˆRoPEè¦æ±‚ï¼‰
        assert self.qk_head_dim % 2 == 0
        
        # K/V æ½œåœ¨æŠ•å½±å‹ç¼©
        self.kv_compress = nn.Linear(args.d_model, args.kv_lora_rank, bias=False)
        self.kv_norm = RMSNorm(args.kv_lora_rank)
        
        # ä»å‹ç¼©è¡¨ç¤ºæ¢å¤ K/V
        self.k_up = nn.Linear(args.kv_lora_rank, self.n_heads * self.qk_head_dim, bias=False)
        self.v_up = nn.Linear(args.kv_lora_rank, self.n_heads * self.v_dim, bias=False)
        
        # æŸ¥è¯¢æŠ•å½±ï¼ˆç”¨äºRoPEï¼‰
        self.q_proj = nn.Linear(args.d_model, self.n_heads * self.qk_head_dim, bias=False)
```

### 2. RoPE (Rotary Position Embedding)

RoPEæ˜¯æœ¬æ¨¡å‹çš„æ ¸å¿ƒä½ç½®ç¼–ç æ–¹æ¡ˆï¼Œåœ¨æ¯ä¸ªæ³¨æ„åŠ›å±‚ä¸­å¯¹Qå’ŒKå‘é‡è¿›è¡Œæ—‹è½¬å˜æ¢ã€‚

**é‡è¦è¯´æ˜**ï¼šRoPEåœ¨Transformerçš„æ³¨æ„åŠ›å±‚ä¸­å®ç°ï¼Œè€Œä¸æ˜¯åœ¨åµŒå…¥å±‚ã€‚è¿™ç§è®¾è®¡æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š
- **èŒè´£åˆ†ç¦»**ï¼šåµŒå…¥å±‚ä¸“æ³¨äºç‰¹å¾è¡¨ç¤ºï¼Œæ³¨æ„åŠ›å±‚å¤„ç†ä½ç½®ä¿¡æ¯
- **ç²¾ç¡®æ§åˆ¶**ï¼šæ¯å±‚éƒ½èƒ½é‡æ–°è®¡ç®—ä½ç½®å…³ç³»ï¼Œå­¦ä¹ ä¸åŒç²’åº¦çš„ä½ç½®æ¨¡å¼
- **ç‰¹å¾çº¯å‡€**ï¼šä¿æŒåµŒå…¥ç‰¹å¾çš„åŸå§‹è¡¨ç¤ºèƒ½åŠ›ï¼Œä¸è¢«ä½ç½®ä¿¡æ¯æ±¡æŸ“

#### 2.1 å·¥ä½œåŸç†

RoPEé€šè¿‡å¤æ•°æ—‹è½¬çš„æ–¹å¼ä¸ºå‘é‡æ·»åŠ ä½ç½®ä¿¡æ¯ï¼š

```python
def precompute_freqs_cis(dim: int, end: int, theta: float = 1e4):
    """
    é¢„è®¡ç®—RoPEçš„é¢‘ç‡å¤æ•°

    Args:
        dim: RoPEç»´åº¦ï¼ˆå¿…é¡»æ˜¯å¶æ•°ï¼‰
        end: æœ€å¤§åºåˆ—é•¿åº¦
        theta: åŸºç¡€é¢‘ç‡å‚æ•°

    Returns:
        é¢‘ç‡å¤æ•°å¼ é‡ [end, dim//2]
    """
    # è®¡ç®—é¢‘ç‡ï¼šÎ¸_i = Î¸^(-2i/d) for i = 0, 1, ..., d/2-1
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))

    # ç”Ÿæˆä½ç½®ç´¢å¼•ï¼šm = 0, 1, 2, ..., end-1
    t = torch.arange(end, device=freqs.device, dtype=freqs.dtype)

    # è®¡ç®—æ¯ä¸ªä½ç½®å’Œé¢‘ç‡çš„å¤–ç§¯ï¼šm * Î¸_i
    freqs = torch.outer(t, freqs).float()

    # è½¬æ¢ä¸ºå¤æ•°å½¢å¼ï¼še^(i * m * Î¸_i) = cos(m*Î¸_i) + i*sin(m*Î¸_i)
    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)

    return freqs_cis

def apply_rotary_emb(xq, xk, freqs_cis):
    """
    å¯¹æŸ¥è¯¢å’Œé”®å¼ é‡åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç 

    Args:
        xq: æŸ¥è¯¢å¼ é‡ [batch_size, seq_len, n_heads, head_dim]
        xk: é”®å¼ é‡ [batch_size, seq_len, n_heads, head_dim]
        freqs_cis: é¢‘ç‡å¤æ•° [seq_len, head_dim//2]

    Returns:
        åº”ç”¨RoPEåçš„(xq, xk)
    """
    # å°†å®æ•°å‘é‡é‡å¡‘ä¸ºå¤æ•°è¡¨ç¤º [batch, seq_len, n_heads, head_dim//2]
    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))
    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))

    # è°ƒæ•´freqs_cisçš„å½¢çŠ¶ä»¥åŒ¹é…è¾“å…¥å¼ é‡
    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)

    # åº”ç”¨æ—‹è½¬ï¼šå¤æ•°ä¹˜æ³•å®ç°æ—‹è½¬å˜æ¢
    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)
    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)

    return xq_out.type_as(xq), xk_out.type_as(xk)

def reshape_for_broadcast(freqs_cis, x):
    """è°ƒæ•´é¢‘ç‡å¤æ•°çš„å½¢çŠ¶ä»¥åŒ¹é…è¾“å…¥å¼ é‡"""
    ndim = x.ndim
    assert 0 <= 1 < ndim
    assert freqs_cis.shape == (x.shape[1], x.shape[-1])
    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]
    return freqs_cis.view(*shape)
```

#### 2.2 æ•°å­¦åŸç†

RoPEçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†ä½ç½®ä¿¡æ¯ç¼–ç ä¸ºæ—‹è½¬çŸ©é˜µï¼š

1. **å¤æ•°è¡¨ç¤º**ï¼šå°†å‘é‡çš„ç›¸é‚»ä¸¤ä¸ªç»´åº¦ç»„åˆæˆå¤æ•°
2. **æ—‹è½¬å˜æ¢**ï¼šé€šè¿‡å¤æ•°ä¹˜æ³•å®ç°æ—‹è½¬
3. **ç›¸å¯¹ä½ç½®**ï¼šä¸¤ä¸ªä½ç½®é—´çš„ç›¸å¯¹è·ç¦»å†³å®šäº†å®ƒä»¬çš„ç›¸å¯¹æ—‹è½¬è§’åº¦

å¯¹äºä½ç½®må’Œnçš„ä¸¤ä¸ªå‘é‡ï¼Œå®ƒä»¬çš„å†…ç§¯å…·æœ‰ç›¸å¯¹ä½ç½®ä¸å˜æ€§ï¼š
```
<RoPE(q_m), RoPE(k_n)> = <q_m, k_n> * e^(i(m-n)Î¸)
```

#### 2.3 åœ¨æ³¨æ„åŠ›ä¸­çš„åº”ç”¨

```python
class MultiHeadLatentAttention(nn.Module):
    def forward(self, x, freqs_cis):
        # 1. è®¡ç®—Q, K, V
        q = self.q_proj(x)  # [batch, seq_len, n_heads * head_dim]

        # é€šè¿‡æ½œåœ¨å‹ç¼©è®¡ç®—K, V
        kv_compressed = self.kv_compress(x)  # [batch, seq_len, kv_lora_rank]
        kv_compressed = self.kv_norm(kv_compressed)

        k = self.k_up(kv_compressed)  # [batch, seq_len, n_heads * head_dim]
        v = self.v_up(kv_compressed)  # [batch, seq_len, n_heads * v_dim]

        # 2. é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼
        q = q.view(batch_size, seq_len, self.n_heads, self.qk_head_dim)
        k = k.view(batch_size, seq_len, self.n_heads, self.qk_head_dim)
        v = v.view(batch_size, seq_len, self.n_heads, self.v_dim)

        # 3. åº”ç”¨RoPEï¼ˆåªå¯¹Qå’ŒKï¼ŒVä¸å˜ï¼‰
        q, k = apply_rotary_emb(q, k, freqs_cis)

        # 4. è®¡ç®—æ³¨æ„åŠ›
        attn_output = scaled_dot_product_attention(q, k, v)

        return attn_output
```

#### 2.4 RoPEä¼˜åŠ¿

**ç›¸æ¯”ä¼ ç»Ÿä½ç½®ç¼–ç çš„ä¼˜åŠ¿**ï¼š

1. **ç›¸å¯¹ä½ç½®ç¼–ç **ï¼š
   - å…³æ³¨tokené—´çš„ç›¸å¯¹è·ç¦»è€Œéç»å¯¹ä½ç½®
   - æ›´é€‚åˆæ—¶é—´åºåˆ—ï¼šæ˜¨å¤©vsä»Šå¤©æ¯”ç¬¬179å¤©vsç¬¬180å¤©æ›´æœ‰æ„ä¹‰

2. **å¤–æ¨èƒ½åŠ›å¼º**ï¼š
   - èƒ½å¤„ç†è®­ç»ƒæ—¶æœªè§è¿‡çš„åºåˆ—é•¿åº¦
   - å¯¹é‡‘èæ•°æ®çš„é•¿åºåˆ—å¤„ç†å¾ˆé‡è¦

3. **æ—‹è½¬ä¸å˜æ€§**ï¼š
   - ä¿æŒå‘é‡çš„å‡ ä½•æ€§è´¨å’Œé•¿åº¦
   - ä¸ä¼šæ”¹å˜ç‰¹å¾çš„åŸå§‹è¡¨ç¤ºèƒ½åŠ›

4. **è®¡ç®—æ•ˆç‡**ï¼š
   - é€šè¿‡å¤æ•°ä¹˜æ³•å®ç°ï¼Œè®¡ç®—é«˜æ•ˆ
   - å¯ä»¥é¢„è®¡ç®—é¢‘ç‡å¤æ•°ï¼Œå‡å°‘è¿è¡Œæ—¶å¼€é”€

**é€‚åˆé‡‘èæ—¶åºçš„åŸå› **ï¼š

1. **æ—¶é—´ç›¸å¯¹æ€§**ï¼šé‡‘èæ•°æ®ä¸­ç›¸å¯¹æ—¶é—´å…³ç³»æ¯”ç»å¯¹æ—¶é—´æ›´é‡è¦
2. **é•¿åºåˆ—æ”¯æŒ**ï¼š180å¤©çš„åºåˆ—é•¿åº¦ï¼ŒRoPEèƒ½å¾ˆå¥½å¤„ç†
3. **æ¨¡å¼è¯†åˆ«**ï¼šç›¸å¯¹ä½ç½®ç¼–ç æœ‰åŠ©äºè¯†åˆ«é‡å¤çš„æ—¶é—´æ¨¡å¼

#### 2.5 é…ç½®å‚æ•°

```python
# RoPEç›¸å…³é…ç½®
rope_theta: float = 10000.0    # åŸºç¡€é¢‘ç‡å‚æ•°ï¼Œæ§åˆ¶æ—‹è½¬é¢‘ç‡
max_seq_len: int = 512         # æ”¯æŒçš„æœ€å¤§åºåˆ—é•¿åº¦
```

**å‚æ•°è¯¦è§£**ï¼š

- `rope_theta`ï¼šåŸºç¡€é¢‘ç‡å‚æ•°ï¼Œå½±å“ä½ç½®ç¼–ç çš„é¢‘ç‡åˆ†å¸ƒ
  - **æ•°å­¦å«ä¹‰**ï¼šÎ¸_i = Î¸^(-2i/d)ï¼ŒÎ¸è¶Šå¤§ï¼Œé«˜ç»´åº¦çš„é¢‘ç‡è¶Šä½
  - **æ•ˆæœ**ï¼šè¶Šå¤§åˆ™ä½é¢‘åˆ†é‡è¶Šå¤šï¼Œä½ç½®ä¿¡æ¯è¡°å‡æ›´æ…¢ï¼Œé€‚åˆé•¿åºåˆ—
  - **é‡‘èåº”ç”¨**ï¼š10000.0é€‚åˆ180å¤©åºåˆ—ï¼Œèƒ½æ•æ‰é•¿æœŸè¶‹åŠ¿
  - **è°ƒä¼˜å»ºè®®**ï¼šåºåˆ—è¶Šé•¿ï¼ŒÎ¸åº”è¯¥è¶Šå¤§

- `max_seq_len`ï¼šé¢„è®¡ç®—é¢‘ç‡å¤æ•°çš„æœ€å¤§é•¿åº¦
  - **ä½œç”¨**ï¼šé¢„å…ˆè®¡ç®—æ‰€æœ‰ä½ç½®çš„æ—‹è½¬å¤æ•°ï¼Œæé«˜è¿è¡Œæ•ˆç‡
  - **è®¾ç½®**ï¼šåº”è¯¥å¤§äºç­‰äºå®é™…ä½¿ç”¨çš„æœ€å¤§åºåˆ—é•¿åº¦
  - **å†…å­˜å½±å“**ï¼šæ›´å¤§çš„å€¼ä¼šå ç”¨æ›´å¤šå†…å­˜

#### 2.6 å®é™…åº”ç”¨ç¤ºä¾‹

```python
# åœ¨PriceTransformerä¸­çš„ä½¿ç”¨
class PriceTransformer(nn.Module):
    def __init__(self, args):
        # é¢„è®¡ç®—RoPEé¢‘ç‡
        self.freqs_cis = precompute_freqs_cis(
            dim=args.d_model // args.n_heads,  # æ¯ä¸ªå¤´çš„ç»´åº¦
            end=args.max_seq_len,              # æœ€å¤§åºåˆ—é•¿åº¦
            theta=args.rope_theta               # åŸºç¡€é¢‘ç‡å‚æ•°
        )

    def forward(self, x):
        # è·å–å½“å‰åºåˆ—é•¿åº¦å¯¹åº”çš„é¢‘ç‡
        seq_len = x.size(1)
        freqs_cis = self.freqs_cis[:seq_len]

        # åœ¨æ¯ä¸ªTransformerå±‚ä¸­åº”ç”¨
        for layer in self.layers:
            x = layer(x, freqs_cis)

        return x
```

#### 2.7 ä¸ä¼ ç»Ÿä½ç½®ç¼–ç å¯¹æ¯”

| ç‰¹æ€§ | RoPE | ä¼ ç»Ÿä½ç½®ç¼–ç  |
|------|------|-------------|
| **ç¼–ç æ–¹å¼** | æ—‹è½¬å˜æ¢ | ç›´æ¥ç›¸åŠ  |
| **ä½ç½®ç±»å‹** | ç›¸å¯¹ä½ç½® | ç»å¯¹ä½ç½® |
| **åº”ç”¨ä½ç½®** | æ³¨æ„åŠ›å±‚ | åµŒå…¥å±‚ |
| **å¤–æ¨èƒ½åŠ›** | å¼º | å¼± |
| **è®¡ç®—å¼€é”€** | æ¯å±‚è®¡ç®— | ä¸€æ¬¡æ€§ |
| **é€‚ç”¨åœºæ™¯** | é•¿åºåˆ—ã€æ—¶é—´åºåˆ— | é€šç”¨åœºæ™¯ |

**ä¸ºä»€ä¹ˆé€‰æ‹©RoPE**ï¼š

1. **é‡‘èæ—¶åºç‰¹æ€§**ï¼šç›¸å¯¹æ—¶é—´å…³ç³»æ¯”ç»å¯¹æ—¶é—´æ›´é‡è¦
2. **é•¿åºåˆ—å¤„ç†**ï¼š180å¤©åºåˆ—ï¼ŒRoPEå¤–æ¨èƒ½åŠ›å¼º
3. **æ¨¡å¼è¯†åˆ«**ï¼šæœ‰åŠ©äºè¯†åˆ«å‘¨æœŸæ€§å’Œè¶‹åŠ¿æ€§æ¨¡å¼
4. **ç°ä»£æ¶æ„**ï¼šä¸MLAç­‰ç°ä»£æ³¨æ„åŠ›æœºåˆ¶é…åˆæ›´å¥½

#### 2.8 ä½¿ç”¨æ³¨æ„äº‹é¡¹

**å¿…è¦æ¡ä»¶**ï¼š
1. **å¤´ç»´åº¦å¿…é¡»æ˜¯å¶æ•°**ï¼šRoPEéœ€è¦å°†å‘é‡ç»´åº¦æˆå¯¹ç»„åˆä¸ºå¤æ•°
2. **é¢„è®¡ç®—é¢‘ç‡**ï¼šéœ€è¦é¢„å…ˆè®¡ç®—æ‰€æœ‰ä½ç½®çš„æ—‹è½¬å¤æ•°
3. **åªå¯¹Qå’ŒKåº”ç”¨**ï¼šVå‘é‡ä¸åº”ç”¨RoPEï¼Œä¿æŒåŸå§‹ç‰¹å¾è¡¨ç¤º

**å¸¸è§é—®é¢˜**ï¼š
1. **ç»´åº¦ä¸åŒ¹é…**ï¼šç¡®ä¿ `d_model // n_heads` æ˜¯å¶æ•°
2. **åºåˆ—é•¿åº¦è¶…é™**ï¼šè¾“å…¥åºåˆ—é•¿åº¦ä¸èƒ½è¶…è¿‡ `max_seq_len`
3. **è®¾å¤‡ä¸åŒ¹é…**ï¼šç¡®ä¿é¢‘ç‡å¤æ•°ä¸è¾“å…¥å¼ é‡åœ¨åŒä¸€è®¾å¤‡ä¸Š

**è°ƒä¼˜å»ºè®®**ï¼š
1. **thetaå‚æ•°**ï¼šæ ¹æ®åºåˆ—é•¿åº¦è°ƒæ•´ï¼Œé•¿åºåˆ—ä½¿ç”¨æ›´å¤§çš„theta
2. **ç¼“å­˜é¢‘ç‡**ï¼šé¢„è®¡ç®—å¹¶ç¼“å­˜é¢‘ç‡å¤æ•°ï¼Œé¿å…é‡å¤è®¡ç®—
3. **å†…å­˜ä¼˜åŒ–**ï¼šåˆç†è®¾ç½®max_seq_lenï¼Œå¹³è¡¡æ€§èƒ½å’Œå†…å­˜ä½¿ç”¨

### 3. SwiGLU å‰é¦ˆç½‘ç»œ

```python
class SwiGLU(nn.Module):
    def forward(self, x):
        gate = F.silu(self.w1(x))    # é—¨æ§åˆ†æ”¯
        value = self.w3(x)           # å€¼åˆ†æ”¯
        hidden = gate * value        # é—¨æ§æœºåˆ¶
        return self.w2(hidden)       # è¾“å‡ºæŠ•å½±
```

**ç‰¹ç‚¹**ï¼š
- ä½¿ç”¨ SiLU (Swish) æ¿€æ´»å‡½æ•°
- é—¨æ§æœºåˆ¶æé«˜è¡¨è¾¾èƒ½åŠ›
- æ¯”ä¼ ç»Ÿ ReLU FFN æ•ˆæœæ›´å¥½

### 4. RMSNorm

```python
class RMSNorm(nn.Module):
    def forward(self, x):
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight
```

**ä¼˜åŠ¿**ï¼š
- è®¡ç®—æ›´ç®€å•ï¼Œæ— éœ€è®¡ç®—å‡å€¼
- è®­ç»ƒæ›´ç¨³å®š
- åœ¨å¤§æ¨¡å‹ä¸­è¡¨ç°æ›´å¥½

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### åˆ›å»ºæ¨¡å‹

```python
from src.price_prediction.price_transformer import PriceTransformer
from src.price_prediction.config import PricePredictionConfigs

# åˆ›å»ºé…ç½®
config = PricePredictionConfigs.base()

# åˆ›å»ºæ¨¡å‹
model = PriceTransformer(config)

# å‰å‘ä¼ æ’­
batch_size, seq_len, n_features = 4, 180, 20
financial_data = torch.randn(batch_size, seq_len, n_features)

outputs = model(financial_data, return_features=True, return_dict=True)
print(f"ä»·æ ¼é¢„æµ‹: {outputs['price_predictions'].shape}")  # [4, 10]
print(f"ç‰¹å¾å‘é‡: {outputs['strategy_features'].shape}")    # [4, 512]
```

### é…ç½®å‚æ•°

```python
# æ¨¡å‹é…ç½®
config = PricePredictionConfigs.base()
config.d_model = 512          # æ¨¡å‹ç»´åº¦
config.n_layers = 8          # Transformerå±‚æ•°
config.n_heads = 8           # æ³¨æ„åŠ›å¤´æ•°
config.kv_lora_rank = 256    # K/Vå‹ç¼©ç»´åº¦
config.intermediate_size = 2048  # FFNéšè—ç»´åº¦

# è®­ç»ƒé…ç½®
config.learning_rate = 1e-4
config.weight_decay = 0.01
config.dropout = 0.1

# RoPEé…ç½®
config.rope_theta = 10000.0
config.max_seq_len = 512
```

### å•ç‹¬ä½¿ç”¨ TransformerBlock

```python
from src.price_prediction.price_transformer import TransformerBlock

# åˆ›å»ºå•ä¸ª Transformer å±‚
transformer_block = TransformerBlock(config, layer_idx=0)

# å‡†å¤‡è¾“å…¥
x = torch.randn(batch_size, seq_len, config.d_model)
freqs_cis = precompute_freqs_cis(
    dim=config.d_model // config.n_heads,
    end=seq_len,
    theta=config.rope_theta
)

# å‰å‘ä¼ æ’­
output = transformer_block(x, freqs_cis)
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")  # [batch_size, seq_len, d_model]
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. å†…å­˜ä¼˜åŒ–

```python
# ä½¿ç”¨MLAå‡å°‘å†…å­˜å ç”¨
traditional_memory = n_heads * seq_len * seq_len * head_dim  # O(nÂ²)
mla_memory = kv_lora_rank * seq_len + n_heads * head_dim     # O(n)

print(f"å†…å­˜èŠ‚çœ: {traditional_memory / mla_memory:.2f}x")

# ç¤ºä¾‹è®¡ç®—
seq_len = 180
n_heads = 8
head_dim = 64
kv_lora_rank = 256

traditional = n_heads * seq_len * seq_len * head_dim  # 663,552,000
mla = kv_lora_rank * seq_len + n_heads * head_dim     # 46,592

print(f"ä¼ ç»Ÿæ³¨æ„åŠ›å†…å­˜: {traditional:,}")
print(f"MLAå†…å­˜: {mla:,}")
print(f"èŠ‚çœæ¯”ä¾‹: {traditional / mla:.1f}x")
```

### 2. è®¡ç®—é€Ÿåº¦

```python
# å‹ç¼©æ¯”é…ç½®
config.kv_lora_rank = 128    # é«˜å‹ç¼©
config.kv_lora_rank = 256    # æ ‡å‡†å‹ç¼©
config.kv_lora_rank = 512    # ä½å‹ç¼©

# FFNé…ç½®
config.intermediate_size = 2 * config.d_model  # è½»é‡çº§
config.intermediate_size = 4 * config.d_model  # æ ‡å‡†é…ç½®
config.intermediate_size = 8 * config.d_model  # é«˜å®¹é‡
```

---

## ğŸ”— ç›¸å…³æ–‡ä»¶

- `src/price_prediction/price_transformer.py` - ä¸»Transformerå®ç°
- `src/price_prediction/feedforward.py` - å‰é¦ˆç½‘ç»œå®ç°
- `src/price_prediction/attention.py` - æ³¨æ„åŠ›æœºåˆ¶å®ç°
- `src/price_prediction/embedding.py` - åµŒå…¥å±‚å®ç°
- `doc/feedforward.md` - å‰é¦ˆç½‘ç»œæ–‡æ¡£

è¿™å¥—Transformeræ¶æ„èƒ½å¤Ÿä¸ºé‡‘èæ—¶åºé¢„æµ‹æä¾›å¼ºå¤§ä¸”é«˜æ•ˆçš„ç¼–ç æ¨¡å‹åŸºç¡€ï¼
