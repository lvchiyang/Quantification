# Transformer æ¶æ„æ–‡æ¡£

ä¸“é—¨ä¸ºé‡‘èæ—¶åºé¢„æµ‹è®¾è®¡çš„ Transformer æ¶æ„ï¼ŒåŒ…å« TransformerBlock å’Œç›¸å…³ç»„ä»¶çš„è¯¦ç»†è¯´æ˜ã€‚

## ğŸ“‹ ç›®å½•

- [æ¶æ„æ¦‚è¿°](#æ¶æ„æ¦‚è¿°)
- [TransformerBlock](#transformerblock)
- [æ ¸å¿ƒç»„ä»¶](#æ ¸å¿ƒç»„ä»¶)
- [ä½¿ç”¨æ–¹æ³•](#ä½¿ç”¨æ–¹æ³•)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)

---

## ğŸ—ï¸ æ¶æ„æ¦‚è¿°

### æ•´ä½“è®¾è®¡ç†å¿µ

æœ¬ Transformer æ¶æ„ä¸“é—¨é’ˆå¯¹é‡‘èæ—¶åºé¢„æµ‹ä»»åŠ¡ä¼˜åŒ–ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

1. **Multi-Head Latent Attention (MLA)**ï¼šé«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶
2. **RoPE ä½ç½®ç¼–ç **ï¼šæ—‹è½¬ä½ç½®ç¼–ç ï¼Œé€‚åˆé•¿åºåˆ—
3. **SwiGLU å‰é¦ˆç½‘ç»œ**ï¼šç°ä»£åŒ–çš„å‰é¦ˆç½‘ç»œè®¾è®¡
4. **Pre-RMSNorm**ï¼šæ›´ç¨³å®šçš„å½’ä¸€åŒ–æ–¹å¼
5. **é‡‘èç‰¹å¾åµŒå…¥**ï¼šä¸“é—¨çš„ç‰¹å¾åˆ†ç»„åµŒå…¥

### æ•°æ®æµ

```
è¾“å…¥: [batch, 180, 20] é‡‘èç‰¹å¾
  â†“
FinancialEmbeddingLayer: åˆ†ç»„åµŒå…¥ + æ‰¹æ ‡å‡†åŒ–
  â†“
[batch, 180, d_model] åµŒå…¥ç‰¹å¾
  â†“
TransformerBlock Ã— n_layers:
  - RMSNorm â†’ MLA + RoPE â†’ æ®‹å·®è¿æ¥
  - RMSNorm â†’ SwiGLU FFN â†’ æ®‹å·®è¿æ¥
  â†“
[batch, 180, d_model] ç¼–ç ç‰¹å¾
  â†“
å–æœ€åæ—¶é—´æ­¥: [batch, d_model]
  â†“
ä»·æ ¼é¢„æµ‹å¤´: [batch, 10] æœªæ¥10ä¸ªæ—¶é—´ç‚¹é¢„æµ‹
```

---

## ğŸ§© TransformerBlock

### è®¾è®¡åŸç†

TransformerBlock æ˜¯ Transformer çš„æ ¸å¿ƒç»„ä»¶ï¼Œé‡‡ç”¨ Pre-RMSNorm ç»“æ„ï¼š

```python
class TransformerBlock(nn.Module):
    """
    ç»“æ„ï¼š
    x -> RMSNorm -> MLA -> Add -> RMSNorm -> FFN -> Add
    """
    
    def __init__(self, args, layer_idx: int = 0):
        super().__init__()
        
        # æ³¨æ„åŠ›å±‚
        self.attn_norm = RMSNorm(args.d_model)
        self.attn = MultiHeadLatentAttention(args)
        
        # å‰é¦ˆç½‘ç»œå±‚
        self.ffn_norm = RMSNorm(args.d_model)
        self.ffn = get_ffn(args, ffn_type="swiglu")
```

### å‰å‘ä¼ æ’­

```python
def forward(self, x, freqs_cis, attn_mask=None, is_causal=False):
    # Pre-RMSNorm + MLA + æ®‹å·®è¿æ¥
    attn_input = self.attn_norm(x)
    attn_output = self.attn(attn_input, freqs_cis, attn_mask, is_causal)
    x = x + attn_output
    
    # Pre-RMSNorm + FFN + æ®‹å·®è¿æ¥
    ffn_input = self.ffn_norm(x)
    ffn_output = self.ffn(ffn_input)
    x = x + ffn_output
    
    return x
```

### å…³é”®ç‰¹æ€§

1. **Pre-RMSNorm**ï¼š
   - åœ¨æ³¨æ„åŠ›å’ŒFFNä¹‹å‰è¿›è¡Œå½’ä¸€åŒ–
   - æ¯” Post-Norm æ›´ç¨³å®šï¼Œè®­ç»ƒæ›´å®¹æ˜“

2. **æ®‹å·®è¿æ¥**ï¼š
   - ç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
   - å…è®¸æ›´æ·±çš„ç½‘ç»œç»“æ„

3. **å±‚ç´¢å¼•**ï¼š
   - æ”¯æŒå±‚ç‰¹å®šçš„é…ç½®
   - ä¾¿äºåˆ†æä¸åŒå±‚çš„ä½œç”¨

---

## ğŸ”§ æ ¸å¿ƒç»„ä»¶

### 1. Multi-Head Latent Attention (MLA)

**ä¼ ç»Ÿæ³¨æ„åŠ› vs MLA**ï¼š

```python
# ä¼ ç»Ÿæ³¨æ„åŠ›
Traditional: Q@K^T@V  # O(nÂ²d) å¤æ‚åº¦

# MLA
MLA: Q@(W_kv@X)^T@(W_kv@X)  # O(nd) å¤æ‚åº¦ï¼Œd<<næ—¶é«˜æ•ˆ

# K/Vå‹ç¼©ç¤ºä¾‹
original_kv_dim = 1024    # åŸå§‹ç»´åº¦
compressed_dim = 256      # å‹ç¼©åç»´åº¦
compression_ratio = 4     # å‹ç¼©æ¯”ä¾‹
```

**MLAå®ç°ç»†èŠ‚**ï¼š

```python
class MultiHeadLatentAttention(nn.Module):
    def __init__(self, args):
        # æŸ¥è¯¢/é”®å¤´ç»´åº¦ï¼Œä½¿ç”¨æ ‡å‡†çš„d_model/n_heads
        self.qk_head_dim = args.d_model // args.n_heads
        
        # ç¡®ä¿å¤´ç»´åº¦æ˜¯å¶æ•°ï¼ˆRoPEè¦æ±‚ï¼‰
        assert self.qk_head_dim % 2 == 0
        
        # K/V æ½œåœ¨æŠ•å½±å‹ç¼©
        self.kv_compress = nn.Linear(args.d_model, args.kv_lora_rank, bias=False)
        self.kv_norm = RMSNorm(args.kv_lora_rank)
        
        # ä»å‹ç¼©è¡¨ç¤ºæ¢å¤ K/V
        self.k_up = nn.Linear(args.kv_lora_rank, self.n_heads * self.qk_head_dim, bias=False)
        self.v_up = nn.Linear(args.kv_lora_rank, self.n_heads * self.v_dim, bias=False)
        
        # æŸ¥è¯¢æŠ•å½±ï¼ˆç”¨äºRoPEï¼‰
        self.q_proj = nn.Linear(args.d_model, self.n_heads * self.qk_head_dim, bias=False)
```

### 2. RoPE (Rotary Position Embedding)

**ä½ç½®ç¼–ç åŸç†**ï¼š

```python
def precompute_freqs_cis(dim: int, end: int, theta: float = 1e4):
    # è®¡ç®—é¢‘ç‡
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))
    
    # è®¡ç®—ä½ç½®ç´¢å¼•
    t = torch.arange(end, device=freqs.device, dtype=freqs.dtype)
    freqs = torch.outer(t, freqs).float()
    
    # è½¬æ¢ä¸ºå¤æ•°å½¢å¼
    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)
    return freqs_cis

def apply_rotary_emb(xq, xk, freqs_cis):
    # é‡å¡‘ä¸ºå¤æ•°
    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))
    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))
    
    # åº”ç”¨æ—‹è½¬
    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)
    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)
    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)
    
    return xq_out.type_as(xq), xk_out.type_as(xk)
```

**RoPE ä¼˜åŠ¿**ï¼š
- ç›¸å¯¹ä½ç½®ç¼–ç ï¼Œé€‚åˆé•¿åºåˆ—
- æ—‹è½¬ä¸å˜æ€§ï¼Œä¿æŒå‘é‡é•¿åº¦
- å¤–æ¨èƒ½åŠ›å¼ºï¼Œå¯å¤„ç†è®­ç»ƒæ—¶æœªè§è¿‡çš„åºåˆ—é•¿åº¦

### 3. SwiGLU å‰é¦ˆç½‘ç»œ

```python
class SwiGLU(nn.Module):
    def forward(self, x):
        gate = F.silu(self.w1(x))    # é—¨æ§åˆ†æ”¯
        value = self.w3(x)           # å€¼åˆ†æ”¯
        hidden = gate * value        # é—¨æ§æœºåˆ¶
        return self.w2(hidden)       # è¾“å‡ºæŠ•å½±
```

**ç‰¹ç‚¹**ï¼š
- ä½¿ç”¨ SiLU (Swish) æ¿€æ´»å‡½æ•°
- é—¨æ§æœºåˆ¶æé«˜è¡¨è¾¾èƒ½åŠ›
- æ¯”ä¼ ç»Ÿ ReLU FFN æ•ˆæœæ›´å¥½

### 4. RMSNorm

```python
class RMSNorm(nn.Module):
    def forward(self, x):
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight
```

**ä¼˜åŠ¿**ï¼š
- è®¡ç®—æ›´ç®€å•ï¼Œæ— éœ€è®¡ç®—å‡å€¼
- è®­ç»ƒæ›´ç¨³å®š
- åœ¨å¤§æ¨¡å‹ä¸­è¡¨ç°æ›´å¥½

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### åˆ›å»ºæ¨¡å‹

```python
from src.price_prediction.price_transformer import PriceTransformer
from src.price_prediction.config import PricePredictionConfigs

# åˆ›å»ºé…ç½®
config = PricePredictionConfigs.base()

# åˆ›å»ºæ¨¡å‹
model = PriceTransformer(config)

# å‰å‘ä¼ æ’­
batch_size, seq_len, n_features = 4, 180, 20
financial_data = torch.randn(batch_size, seq_len, n_features)

outputs = model(financial_data, return_features=True, return_dict=True)
print(f"ä»·æ ¼é¢„æµ‹: {outputs['price_predictions'].shape}")  # [4, 10]
print(f"ç‰¹å¾å‘é‡: {outputs['strategy_features'].shape}")    # [4, 512]
```

### é…ç½®å‚æ•°

```python
# æ¨¡å‹é…ç½®
config = PricePredictionConfigs.base()
config.d_model = 512          # æ¨¡å‹ç»´åº¦
config.n_layers = 8          # Transformerå±‚æ•°
config.n_heads = 8           # æ³¨æ„åŠ›å¤´æ•°
config.kv_lora_rank = 256    # K/Vå‹ç¼©ç»´åº¦
config.intermediate_size = 2048  # FFNéšè—ç»´åº¦

# è®­ç»ƒé…ç½®
config.learning_rate = 1e-4
config.weight_decay = 0.01
config.dropout = 0.1

# RoPEé…ç½®
config.rope_theta = 10000.0
config.max_seq_len = 512
```

### å•ç‹¬ä½¿ç”¨ TransformerBlock

```python
from src.price_prediction.price_transformer import TransformerBlock

# åˆ›å»ºå•ä¸ª Transformer å±‚
transformer_block = TransformerBlock(config, layer_idx=0)

# å‡†å¤‡è¾“å…¥
x = torch.randn(batch_size, seq_len, config.d_model)
freqs_cis = precompute_freqs_cis(
    dim=config.d_model // config.n_heads,
    end=seq_len,
    theta=config.rope_theta
)

# å‰å‘ä¼ æ’­
output = transformer_block(x, freqs_cis)
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")  # [batch_size, seq_len, d_model]
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. å†…å­˜ä¼˜åŒ–

```python
# ä½¿ç”¨MLAå‡å°‘å†…å­˜å ç”¨
traditional_memory = n_heads * seq_len * seq_len * head_dim  # O(nÂ²)
mla_memory = kv_lora_rank * seq_len + n_heads * head_dim     # O(n)

print(f"å†…å­˜èŠ‚çœ: {traditional_memory / mla_memory:.2f}x")

# ç¤ºä¾‹è®¡ç®—
seq_len = 180
n_heads = 8
head_dim = 64
kv_lora_rank = 256

traditional = n_heads * seq_len * seq_len * head_dim  # 663,552,000
mla = kv_lora_rank * seq_len + n_heads * head_dim     # 46,592

print(f"ä¼ ç»Ÿæ³¨æ„åŠ›å†…å­˜: {traditional:,}")
print(f"MLAå†…å­˜: {mla:,}")
print(f"èŠ‚çœæ¯”ä¾‹: {traditional / mla:.1f}x")
```

### 2. è®¡ç®—é€Ÿåº¦

```python
# å‹ç¼©æ¯”é…ç½®
config.kv_lora_rank = 128    # é«˜å‹ç¼©
config.kv_lora_rank = 256    # æ ‡å‡†å‹ç¼©
config.kv_lora_rank = 512    # ä½å‹ç¼©

# FFNé…ç½®
config.intermediate_size = 2 * config.d_model  # è½»é‡çº§
config.intermediate_size = 4 * config.d_model  # æ ‡å‡†é…ç½®
config.intermediate_size = 8 * config.d_model  # é«˜å®¹é‡
```

---

## ğŸ”— ç›¸å…³æ–‡ä»¶

- `src/price_prediction/price_transformer.py` - ä¸»Transformerå®ç°
- `src/price_prediction/feedforward.py` - å‰é¦ˆç½‘ç»œå®ç°
- `src/price_prediction/attention.py` - æ³¨æ„åŠ›æœºåˆ¶å®ç°
- `src/price_prediction/embedding.py` - åµŒå…¥å±‚å®ç°
- `doc/feedforward.md` - å‰é¦ˆç½‘ç»œæ–‡æ¡£

è¿™å¥—Transformeræ¶æ„èƒ½å¤Ÿä¸ºé‡‘èæ—¶åºé¢„æµ‹æä¾›å¼ºå¤§ä¸”é«˜æ•ˆçš„ç¼–ç æ¨¡å‹åŸºç¡€ï¼
